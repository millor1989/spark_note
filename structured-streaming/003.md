### Structured Streaming集成Kafka

支持Kafka broker的版本为0.10.0或以上

#### 依赖

对于使用SBT的Scala/Java应用：

```
groupId = org.apache.spark
artifactId = spark-sql-kafka-0-10_2.11
version = 2.2.0
```

#### 1、从Kafka读取数据

##### 1.1、创建一个用于流查询的Kafka数据源

```scala
// Subscribe to 1 topic
val df = spark
  .readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .option("subscribe", "topic1")
  .load()
df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
  .as[(String, String)]

// Subscribe to multiple topics
val df = spark
  .readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .option("subscribe", "topic1,topic2")
  .load()
df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
  .as[(String, String)]

// Subscribe to a pattern
val df = spark
  .readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .option("subscribePattern", "topic.*")
  .load()
df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
  .as[(String, String)]
```

##### 1.2、创建一个用于批查询的Kafka数据源

```scala
// Subscribe to 1 topic defaults to the earliest and latest offsets
val df = spark
  .read
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .option("subscribe", "topic1")
  .load()
df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
  .as[(String, String)]

// Subscribe to multiple topics, specifying explicit Kafka offsets
val df = spark
  .read
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .option("subscribe", "topic1,topic2")
  .option("startingOffsets", """{"topic1":{"0":23,"1":-2},"topic2":{"0":-2}}""")
  .option("endingOffsets", """{"topic1":{"0":50,"1":-1},"topic2":{"0":-1}}""")
  .load()
df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
  .as[(String, String)]

// Subscribe to a pattern, at the earliest and latest offsets
val df = spark
  .read
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .option("subscribePattern", "topic.*")
  .option("startingOffsets", "earliest")
  .option("endingOffsets", "latest")
  .load()
df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
  .as[(String, String)]
```

数据源中每一行的schema为：

| 字段          | 类型   |
| ------------- | ------ |
| key           | binary |
| value         | binary |
| topic         | string |
| partition     | int    |
| offset        | long   |
| timestamp     | long   |
| timestampType | int    |

对于Kafka数据源，不管是批查询还是流查询都必须指定如下选项：

| Option                  | value                                     | 含义                                                         |
| ----------------------- | ----------------------------------------- | ------------------------------------------------------------ |
| assign                  | json字符串{"topicA":[0,1],"topicB":[2,4]} | 指定消费的主题分区。对于Kafka数据源，“assign”，“subscribe”或“subscribePattern”三个选项只能指定一个。 |
| subscribe               | 逗号分隔得主题集合                        | 要订阅的主题集合。对于Kafka数据源，“assign”，“subscribe”或“subscribePattern”三个选项只能指定一个。 |
| subscribePattern        | Java 正则字符串                           | 用于订阅主题的模式。对于Kafka数据源，“assign”，“subscribe”或“subscribePattern”三个选项只能指定一个。 |
| kafka.bootstrap.servers | 逗号分隔的`host:port`集合                 | Kafka “bootstrap.servers”配置                                |

如下选项是可选的：

| Option                      | value                                                        | default                          | 查询类型 | 含义                                                         |
| --------------------------- | ------------------------------------------------------------ | -------------------------------- | -------- | ------------------------------------------------------------ |
| startingOffsets             | “earliest”，“latest”（仅流适用），或者json字符串{"topicA":{"0":23,"1":-1},"topicB":{"0":-2}} | 流查询“latest”，批查询“earliest” | 流和批   | 启动查询时的开始位置，可以是“earliest”从最早的偏移开始，“latest”从最新的偏移开始，或者指定了每个主题分区起始偏移的json字符串。在json中，-2用作偏移指的是最早的偏移，-1指的是最新的偏移。注意：对于批查询，不能使用最新的偏移。对于流查询，当新的查询启动时这个设置才会应用，重新开始（resuming）总是从查询离开（left off）的地方继续。查询过程中新发现的分区将从最早的开始。 |
| endingOffset                | “latest”或者json字符串{"topicA":{"0":23,"1":-1},"topicB":{"0":-1}} | “latest”                         | 批查询   | 批查询的结束点。“latest”指的是最新的，json字符串指定每个主题分区的结束偏移。在json中，-1是指最新的，不能使用-2（最早的） |
| failOnDataLoss              | true或false                                                  | true                             | 流查询   | 当有可能数据丢失了时（比如，主题被删除，或者偏移超出范围），将查询失败。这可能是一个false报警。可以在不符合期望时关闭它。对于批查询，如果因为数据丢失它不能从指定的偏移读取到数据，它将会失败。 |
| kafkaConsumer.pollTimeoutMs | long                                                         | 512                              | 流或批   | executors从Kafka poll数据的超时时间，毫秒                    |
| fetchOffset.numRetries      | int                                                          | 3                                | 流或批   | 获取Kafka 偏移的重试次数                                     |
| fetchOffset.retryIntervalMs | long                                                         | 10                               | 流或批   | 重试获取Kafka偏移前等待的时间，毫秒                          |
| maxOffsetsPerTrigger        | long                                                         | none                             | 流或批   | 每个触发间隔处理的最大偏移数量的速率。指定的偏移总数将在不同容量的主题分区之间按比例分配 |

#### 2、往Kafka写数据

本节介绍将 Streaming Queries 和 Batch Queries 写到 Apache Kafka。要注意的是，Apache Kafka 只支持至少一次（at least once）的写语义。所以，当写入 Kafka 的时候，无论是 Streaming 还是 Batch，有些记录可能会重复；比如，当 Kafka 重试没有被 Kafka Broker 确认（acknowledged）的消息时，即使那个 Kafka Broker 已经接收到并写过这条消息，还是会发生重复。Structured Streaming 不能防止这些重复的发生，因为这是 Kafka 的写入语义。但是，如果如果查询写入成功，就可以认为查询输出被至少一次地写过了。一个移除重复记录的可能方案是，当读取写入的数据时，进行读时的去重。

写到 Kafka 的 DataFrame schema 应该具有如下几列：

| Column            | Type             |
| ----------------- | ---------------- |
| key（可选的）     | string or binary |
| value（必须的）   | string or binary |
| topic（※ 可选的） | string           |

※ 如果配置选项中没有指定 “topic”，那么 `topic` 列是必须的。

`value` 列是唯一必需的。如果 `key` 列没有指定，会自动分配一个值为 `null` 的列（可以查看 Kafka 语义来了解如何处理值为 `null` 的 `key`）。如果存在 `topic` 列，那么使用指定的 `topic` 来写数据，但是 “topic” 配置会覆盖DataFrame的 `topic` 列。

无论是批查询还是流查询都必须指定 `kafka.bootstrap.servers` 选项。

##### 2.1、为流查询创建一个 Kafka Sink

```scala
// Write key-value data from a DataFrame to a specific Kafka topic specified in an option
val ds = df
  .selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
  .writeStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .option("topic", "topic1")
  .start()

// Write key-value data from a DataFrame to Kafka using a topic specified in the data
val ds = df
  .selectExpr("topic", "CAST(key AS STRING)", "CAST(value AS STRING)")
  .writeStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .start()
```

##### 2.2、将批查询输出写到 Kafka

```scala
// Write key-value data from a DataFrame to a specific Kafka topic specified in an option
df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
  .write
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .option("topic", "topic1")
  .save()

// Write key-value data from a DataFrame to Kafka using a topic specified in the data
df.selectExpr("topic", "CAST(key AS STRING)", "CAST(value AS STRING)")
  .write
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .save()
```

#### 3、Kakfa 专用配置

通过 `DataStreamReader.option` 加上 `kafka.` 前缀可以使用 Kafka 自有的配置，比如 `sream.option("kafak.bootstrap.servers", "host:port")`。可以用的 Kafka 参数，对于读取数据查看 Kafka 消费者相关的配置参数文档，对于写数据查看 Kafka 生产者相关的配置文档。

注意，**不能设置如下 Kafka 参数**，否则 Kafka Source 或者 Sink 会抛出异常：

- `group.id`：Kafka source 会自动地为每一个查询创建一个唯一的 group id
- `auto.offset.reset`：使用 `startingOffsets` 这个数据源选项来决定从哪里开始消费。Structrued Streaming 内部会管理消费过哪些偏移，不依赖 Kafka 消费者进行管理。这样能够确保在动态地订阅新的 topics/partitions 时不会丢失数据。注意，`startingOffsets` 只用于新的流查询启动的时候，再次提交时将总是从查询上次离开的地方开始消费。
- `key.deserializer`：总是使用 `ByteArrayDeserializer` 将 Keys 反序列化为字节数组。使用 DataFrame 操作来明确地反序列化 keys。
- `value.deserializer`：总是使用 `ByteArrayDeserializer` 将 values 反序列化为字节数组。使用 DataFrame 操作来明确地反序列化 values。
- `key.serializer`：总是使用 `ByteArraySerializer` 或 `StringSerializer` 来序列化 Keys。使用 DataFrame 操作来明确地将 keys 序列化为字符串或字节数组。
- `value.serializer`：总是使用 `ByteArraySerializer` 或 `StringSerializer` 来序列化 Values。使用 DataFrame 操作来明确地将 values 序列化为字符串或字节数组。
- `enable.auto.commit`：Kafka 数据源不会提交任何偏移量。正因如此，使用 Kafka 命令 `kafka-consumer-groups --describe --bootstrap-server 172.16.1.111:9092 --group <group-id> ` 来查看 Structured Streaming 的 Kafka 消费进度时，是没有返回结果的。
- `interceptor.classes`：Kafka 数据源总是将 keys 和 values 读取为字节数组。使用 `ConsumerInterceptor` 是不安全的，这样可能会打断查询。

#### 4、部署

就像 Spark 应用一样，使用 `spark-submit` 来提交应用。可以使用 `--packages` 将 `spark-sql-kafka-0-10_2.11` 和它的依赖添加到 `spark-submit`，例如：

```shell
./bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0 ...
```

