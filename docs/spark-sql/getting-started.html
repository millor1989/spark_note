
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>Getting started · GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-katex/katex.min.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="shu-ju-yuan.html" />
    
    
    <link rel="prev" href="../spark-sql.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Spark总览
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../kuai-su-ru-men.html">
            
                <a href="../kuai-su-ru-men.html">
            
                    
                    快速入门
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../sparkbian-cheng-dao-yin.html">
            
                <a href="../sparkbian-cheng-dao-yin.html">
            
                    
                    Spark编程导引
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="../sparkbian-cheng-dao-yin/rdds.html">
            
                <a href="../sparkbian-cheng-dao-yin/rdds.html">
            
                    
                    RDDs
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2" data-path="../sparkbian-cheng-dao-yin/cong-java-scalaqi-dong-spark-jobs.html">
            
                <a href="../sparkbian-cheng-dao-yin/cong-java-scalaqi-dong-spark-jobs.html">
            
                    
                    从Java/Scala启动Spark jobs
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="../spark-sql.html">
            
                <a href="../spark-sql.html">
            
                    
                    Spark SQL
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter active" data-level="1.4.1" data-path="getting-started.html">
            
                <a href="getting-started.html">
            
                    
                    Getting started
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2" data-path="shu-ju-yuan.html">
            
                <a href="shu-ju-yuan.html">
            
                    
                    数据源
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.3" data-path="xing-neng-diao-shi.html">
            
                <a href="xing-neng-diao-shi.html">
            
                    
                    性能调试
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.4" data-path="fen-bu-shi-sql-yin-qing.html">
            
                <a href="fen-bu-shi-sql-yin-qing.html">
            
                    
                    分布式SQL引擎
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.5" data-path="yu-apache-hive-de-jian-rong-xing.html">
            
                <a href="yu-apache-hive-de-jian-rong-xing.html">
            
                    
                    与Apache Hive的兼容性
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.6" data-path="shu-ju-lei-xing.html">
            
                <a href="shu-ju-lei-xing.html">
            
                    
                    数据类型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.7" data-path="built-in-functions.html">
            
                <a href="built-in-functions.html">
            
                    
                    内置函数
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="../spark-streaming.html">
            
                <a href="../spark-streaming.html">
            
                    
                    Spark Streaming
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.5.1" data-path="../spark-streaming/ji-ben-gai-nian.html">
            
                <a href="../spark-streaming/ji-ben-gai-nian.html">
            
                    
                    基本概念
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.2" data-path="../spark-streaming/xing-neng-diao-shi.html">
            
                <a href="../spark-streaming/xing-neng-diao-shi.html">
            
                    
                    性能调试
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.3" data-path="../spark-streaming/rong-cuo.html">
            
                <a href="../spark-streaming/rong-cuo.html">
            
                    
                    容错
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="../structured-streaming.html">
            
                <a href="../structured-streaming.html">
            
                    
                    Structured Streaming
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.6.1" data-path="../structured-streaming/shi-yong-datasets-hedataframes-de-api.html">
            
                <a href="../structured-streaming/shi-yong-datasets-hedataframes-de-api.html">
            
                    
                    使用Datasets和DataFrames的API
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.2" data-path="../structured-streaming/structured-streaminghe-spark-streaming.html">
            
                <a href="../structured-streaming/structured-streaminghe-spark-streaming.html">
            
                    
                    Structured Streaming和Spark Streaming
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.7" data-path="../bu-shu.html">
            
                <a href="../bu-shu.html">
            
                    
                    部署和运行
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.7.1" data-path="../bu-shu/zu-jian-ff08-components.html">
            
                <a href="../bu-shu/zu-jian-ff08-components.html">
            
                    
                    组件（Components）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.2" data-path="../bu-shu/ji-qun-guan-li-qi-lei-xing.html">
            
                <a href="../bu-shu/ji-qun-guan-li-qi-lei-xing.html">
            
                    
                    集群管理器类型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.3" data-path="../bu-shu/ti-jiao-ying-yong.html">
            
                <a href="../bu-shu/ti-jiao-ying-yong.html">
            
                    
                    提交应用
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.4" data-path="../bu-shu/jian-kong.html">
            
                <a href="../bu-shu/jian-kong.html">
            
                    
                    监控
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.5" data-path="../zuo-ye-diao-du.html">
            
                <a href="../zuo-ye-diao-du.html">
            
                    
                    作业调度
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.6" data-path="../zai-yarn-shang-yun-xing-spark.html">
            
                <a href="../zai-yarn-shang-yun-xing-spark.html">
            
                    
                    在YARN上运行Spark
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.8" data-path="../pei-zhi.html">
            
                <a href="../pei-zhi.html">
            
                    
                    配置
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.8.1" data-path="../pei-zhi/sparkshu-xing.html">
            
                <a href="../pei-zhi/sparkshu-xing.html">
            
                    
                    Spark属性
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8.2" data-path="../pei-zhi/dong-tai-jia-zai-spark-shu-xing.html">
            
                <a href="../pei-zhi/dong-tai-jia-zai-spark-shu-xing.html">
            
                    
                    动态加载Spark属性
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8.3" data-path="../pei-zhi/cha-kan-spark-shu-xing.html">
            
                <a href="../pei-zhi/cha-kan-spark-shu-xing.html">
            
                    
                    查看Spark属性
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.9" data-path="../diao-shi.html">
            
                <a href="../diao-shi.html">
            
                    
                    调试
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.10" data-path="../extff1a-xing-neng-you-hua.html">
            
                <a href="../extff1a-xing-neng-you-hua.html">
            
                    
                    性能优化
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.10.1" data-path="../extff1a-xing-neng-you-hua/ji-chu-you-hua.html">
            
                <a href="../extff1a-xing-neng-you-hua/ji-chu-you-hua.html">
            
                    
                    基础优化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.10.2" data-path="../extff1a-xing-neng-you-hua/gao-ji-you-hua.html">
            
                <a href="../extff1a-xing-neng-you-hua/gao-ji-you-hua.html">
            
                    
                    高级优化
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.11" data-path="../zai-yarn-shang-yun-xing-spark.html">
            
                <a href="../zai-yarn-shang-yun-xing-spark.html">
            
                    
                    在Yarn上运行Spark
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.12" data-path="../zuo-ye-diao-du.html">
            
                <a href="../zuo-ye-diao-du.html">
            
                    
                    作业调度
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.13" data-path="../an-quan.html">
            
                <a href="../an-quan.html">
            
                    
                    安全
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.14" data-path="../ying-jian-pei-zhi.html">
            
                <a href="../ying-jian-pei-zhi.html">
            
                    
                    硬件配置
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15" data-path="../mllib.html">
            
                <a href="../mllib.html">
            
                    
                    MLlib
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.15.1" data-path="../mllib/shu-ju-lei-xing.html">
            
                <a href="../mllib/shu-ju-lei-xing.html">
            
                    
                    数据类型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.2" data-path="../mllib/ji-chu-tong-ji.html">
            
                <a href="../mllib/ji-chu-tong-ji.html">
            
                    
                    基础统计
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.3" data-path="../mllib/pipelines.html">
            
                <a href="../mllib/pipelines.html">
            
                    
                    Pipelines
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.4" data-path="../mllib/te-zheng-de-huo-qu-3001-zhuan-huan-he-shai-xuan.html">
            
                <a href="../mllib/te-zheng-de-huo-qu-3001-zhuan-huan-he-shai-xuan.html">
            
                    
                    特征的提取、转换和筛选
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.15.4.1" data-path="../mllib/te-zheng-de-huo-qu-3001-zhuan-huan-he-shai-xuan/feature-extractors.html">
            
                <a href="../mllib/te-zheng-de-huo-qu-3001-zhuan-huan-he-shai-xuan/feature-extractors.html">
            
                    
                    Feature Extractors
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.4.2" data-path="../mllib/te-zheng-de-huo-qu-3001-zhuan-huan-he-shai-xuan/feature-tranformers.html">
            
                <a href="../mllib/te-zheng-de-huo-qu-3001-zhuan-huan-he-shai-xuan/feature-tranformers.html">
            
                    
                    Feature Transformers-1
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.4.3" data-path="../mllib/te-zheng-de-huo-qu-3001-zhuan-huan-he-shai-xuan/feature-transformers-2.html">
            
                <a href="../mllib/te-zheng-de-huo-qu-3001-zhuan-huan-he-shai-xuan/feature-transformers-2.html">
            
                    
                    Feature Transformers-2
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.4.4" data-path="../mllib/te-zheng-de-huo-qu-3001-zhuan-huan-he-shai-xuan/feature-selectors.html">
            
                <a href="../mllib/te-zheng-de-huo-qu-3001-zhuan-huan-he-shai-xuan/feature-selectors.html">
            
                    
                    Feature Selectors
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.4.5" data-path="../mllib/te-zheng-de-huo-qu-3001-zhuan-huan-he-shai-xuan/locality-sensitive-hashing.html">
            
                <a href="../mllib/te-zheng-de-huo-qu-3001-zhuan-huan-he-shai-xuan/locality-sensitive-hashing.html">
            
                    
                    Locality Sensitive Hashing
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.15.5" data-path="../mllib/fen-lei-he-hui-gui.html">
            
                <a href="../mllib/fen-lei-he-hui-gui.html">
            
                    
                    分类和回归
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.15.5.1" data-path="../mllib/fen-lei-he-hui-gui/fen-lei.html">
            
                <a href="../mllib/fen-lei-he-hui-gui/fen-lei.html">
            
                    
                    分类
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.5.2" data-path="../mllib/fen-lei-he-hui-gui/hui-gui.html">
            
                <a href="../mllib/fen-lei-he-hui-gui/hui-gui.html">
            
                    
                    回归
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.5.3" data-path="../mllib/fen-lei-he-hui-gui/xian-xing-fang-fa.html">
            
                <a href="../mllib/fen-lei-he-hui-gui/xian-xing-fang-fa.html">
            
                    
                    线性方法
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.5.4" data-path="../mllib/fen-lei-he-hui-gui/jue-ce-shu.html">
            
                <a href="../mllib/fen-lei-he-hui-gui/jue-ce-shu.html">
            
                    
                    决策树
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.5.5" data-path="../mllib/fen-lei-he-hui-gui/shu-tuan-ti.html">
            
                <a href="../mllib/fen-lei-he-hui-gui/shu-tuan-ti.html">
            
                    
                    树团体
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.15.6" data-path="../mllib/ju-lei.html">
            
                <a href="../mllib/ju-lei.html">
            
                    
                    聚类
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.7" data-path="../mllib/xie-tong-guo-lv.html">
            
                <a href="../mllib/xie-tong-guo-lv.html">
            
                    
                    协同过滤
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.16" data-path="../questions.html">
            
                <a href="../questions.html">
            
                    
                    问题记录
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.16.1" data-path="../questions/datasetdataframerdd.html">
            
                <a href="../questions/datasetdataframerdd.html">
            
                    
                    DataFrame-Dataset-RDD
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.16.2" data-path="../questions/spark-checkpoint.html">
            
                <a href="../questions/spark-checkpoint.html">
            
                    
                    checkpoint
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.16.3" data-path="../questions/shu-ju-ben-di-xing.html">
            
                <a href="../questions/shu-ju-ben-di-xing.html">
            
                    
                    某个task很慢
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.16.4" data-path="../questions/sparkhan-shu.html">
            
                <a href="../questions/sparkhan-shu.html">
            
                    
                    Spark函数
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.16.5" data-path="../questions/hadoop-1he-hadoop2.html">
            
                <a href="../questions/hadoop-1he-hadoop2.html">
            
                    
                    Hadoop 1和Hadoop2
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.16.6" data-path="../questions/broadcast-join.html">
            
                <a href="../questions/broadcast-join.html">
            
                    
                    Broadcast Join
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >Getting started</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h4 id="1&#x3001;&#x5165;&#x53E3;&#xFF1A;sparksession">1&#x3001;&#x5165;&#x53E3;&#xFF1A;SparkSession</h4>
<p>Spark&#x7684;&#x6240;&#x6709;&#x529F;&#x80FD;&#x7684;&#x5165;&#x53E3;&#x662F;<code>SparkSession</code>&#x7C7B;&#x3002;&#x4F7F;&#x7528;<code>SparkSession.builder()</code>&#x521B;&#x5EFA;&#xFF1A;</p>
<pre><code class="lang-scala"><span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">SparkSession</span>

<span class="hljs-keyword">val</span> spark = <span class="hljs-type">SparkSession</span>
  .builder()
  .appName(<span class="hljs-string">&quot;Spark SQL basic example&quot;</span>)
  .config(<span class="hljs-string">&quot;spark.some.config.option&quot;</span>, <span class="hljs-string">&quot;some-value&quot;</span>)
  .getOrCreate()

<span class="hljs-comment">// For implicit conversions like converting RDDs to DataFrames</span>
<span class="hljs-keyword">import</span> spark.implicits._
</code></pre>
<p>Spark 2.0&#x7684;<code>SparkSession</code>&#x63D0;&#x4F9B;&#x4E86;&#x5BF9;&#x4E8E;Hive&#x7279;&#x5F81;&#xFF08;&#x5305;&#x62EC;&#x4F7F;&#x7528;HiveQL&#x5199;&#x67E5;&#x8BE2;&#x8BED;&#x53E5;&#x7684;&#x80FD;&#x529B;&#xFF0C;&#x8BBF;&#x95EE;Hive&#x7684;UDFs&#xFF0C;&#x4ECE;Hive&#x8868;&#x8BFB;&#x53D6;&#x6570;&#x636E;&#x7684;&#x80FD;&#x529B;&#xFF09;&#x7684;&#x5185;&#x90E8;&#x652F;&#x6301;&#x3002;&#x4E0D;&#x7528;&#x8FDB;&#x884C;Hive&#x8BBE;&#x7F6E;&#x5C31;&#x80FD;&#x4F7F;&#x7528;&#x8FD9;&#x4E9B;&#x7279;&#x5F81;&#x3002;</p>
<h4 id="2&#x3001;&#x521B;&#x5EFA;dataframes">2&#x3001;&#x521B;&#x5EFA;DataFrames</h4>
<p>&#x4F7F;&#x7528;<code>SparkSession</code>&#xFF0C;&#x5E94;&#x7528;&#x53EF;&#x4EE5;&#x4ECE;RDDs&#x3001;Hive&#x8868;&#x6216;&#x8005;&#x5176;&#x5B83;Spark&#x652F;&#x6301;&#x7684;&#x6570;&#x636E;&#x6E90;&#x521B;&#x5EFA;DataFrames&#x3002;&#x6BD4;&#x5982;&#xFF0C;&#x4ECE;JSON&#x6587;&#x4EF6;&#x521B;&#x5EFA;DataFrame&#xFF1A;</p>
<pre><code class="lang-scala"><span class="hljs-keyword">val</span> df = spark.read.json(<span class="hljs-string">&quot;examples/src/main/resources/people.json&quot;</span>)

<span class="hljs-comment">// Displays the content of the DataFrame to stdout</span>
df.show()
<span class="hljs-comment">// +----+-------+</span>
<span class="hljs-comment">// | age|   name|</span>
<span class="hljs-comment">// +----+-------+</span>
<span class="hljs-comment">// |null|Michael|</span>
<span class="hljs-comment">// |  30|   Andy|</span>
<span class="hljs-comment">// |  19| Justin|</span>
<span class="hljs-comment">// +----+-------+</span>
</code></pre>
<h4 id="3&#x3001;&#x65E0;&#x7C7B;&#x578B;dataset-operations&#xFF08;aka-dataframe-operations&#xFF09;">3&#x3001;&#x65E0;&#x7C7B;&#x578B;Dataset Operations&#xFF08;aka DataFrame Operations&#xFF09;</h4>
<p>DataFrames&#x63D0;&#x4F9B;&#x4E86;Scala&#x3001;Java&#x3001;Python&#x548C;R&#x8BED;&#x8A00;&#x7684;DSL&#xFF08;domain-specific language&#xFF0C;&#x9886;&#x57DF;&#x4E13;&#x7528;&#x8BED;&#x8A00;&#xFF09;&#x7528;&#x4E8E;&#x7ED3;&#x6784;&#x5316;&#x6570;&#x636E;&#x7684;&#x5904;&#x7406;&#x3002;</p>
<p>Spark 2.0&#x7684;Java&#x548C;Scala API&#x4E2D;&#xFF0C;DataFrame&#x5C31;&#x662F;<code>Row</code>s&#x7EC4;&#x6210;&#x7684;Dataset&#x3002;&#x4E0E;&#x5F3A;&#x7C7B;&#x578B;&#x7684;Scala/Java&#x6570;&#x636E;&#x96C6;&#x7684;&#x201C;typed transfromations&#x201D;&#x76F8;&#x53CD;&#xFF0C;DataFrame&#x7684;&#x64CD;&#x4F5C;&#x4E5F;&#x53EB;&#x505A;&#x201C;untyped transformations&#x201D;&#x3002;</p>
<pre><code class="lang-scala"><span class="hljs-comment">// This import is needed to use the $-notation</span>
<span class="hljs-keyword">import</span> spark.implicits._
<span class="hljs-comment">// Print the schema in a tree format</span>
df.printSchema()
<span class="hljs-comment">// root</span>
<span class="hljs-comment">// |-- age: long (nullable = true)</span>
<span class="hljs-comment">// |-- name: string (nullable = true)</span>

<span class="hljs-comment">// Select only the &quot;name&quot; column</span>
df.select(<span class="hljs-string">&quot;name&quot;</span>).show()
<span class="hljs-comment">// +-------+</span>
<span class="hljs-comment">// |   name|</span>
<span class="hljs-comment">// +-------+</span>
<span class="hljs-comment">// |Michael|</span>
<span class="hljs-comment">// |   Andy|</span>
<span class="hljs-comment">// | Justin|</span>
<span class="hljs-comment">// +-------+</span>

<span class="hljs-comment">// Select everybody, but increment the age by 1</span>
df.select($<span class="hljs-string">&quot;name&quot;</span>, $<span class="hljs-string">&quot;age&quot;</span> + <span class="hljs-number">1</span>).show()
<span class="hljs-comment">// +-------+---------+</span>
<span class="hljs-comment">// |   name|(age + 1)|</span>
<span class="hljs-comment">// +-------+---------+</span>
<span class="hljs-comment">// |Michael|     null|</span>
<span class="hljs-comment">// |   Andy|       31|</span>
<span class="hljs-comment">// | Justin|       20|</span>
<span class="hljs-comment">// +-------+---------+</span>

<span class="hljs-comment">// Select people older than 21</span>
df.filter($<span class="hljs-string">&quot;age&quot;</span> &gt; <span class="hljs-number">21</span>).show()
<span class="hljs-comment">// +---+----+</span>
<span class="hljs-comment">// |age|name|</span>
<span class="hljs-comment">// +---+----+</span>
<span class="hljs-comment">// | 30|Andy|</span>
<span class="hljs-comment">// +---+----+</span>

<span class="hljs-comment">// Count people by age</span>
df.groupBy(<span class="hljs-string">&quot;age&quot;</span>).count().show()
<span class="hljs-comment">// +----+-----+</span>
<span class="hljs-comment">// | age|count|</span>
<span class="hljs-comment">// +----+-----+</span>
<span class="hljs-comment">// |  19|    1|</span>
<span class="hljs-comment">// |null|    1|</span>
<span class="hljs-comment">// |  30|    1|</span>
<span class="hljs-comment">// +----+-----+</span>
</code></pre>
<p>&#x5728;<a href="http://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank">Dataset API &#x6587;&#x6863;</a>&#x53EF;&#x4EE5;&#x67E5;&#x770B;Dataset&#x5168;&#x90E8;&#x7684;&#x64CD;&#x4F5C;&#x3002;</p>
<p>&#x53E6;&#x5916;&#xFF0C;&#x9664;&#x4E86;&#x7B80;&#x5355;&#x7684;&#x5217;&#x5F15;&#x7528;&#x548C;&#x8868;&#x8FBE;&#x5F0F;&#xFF0C;Datasets&#x8FD8;&#x6709;&#x51FD;&#x6570;&#x5E93;&#x5305;&#x542B;&#x4E86;&#x5B57;&#x7B26;&#x4E32;&#x64CD;&#x4F5C;&#x3001;&#x65E5;&#x524D;&#x8BA1;&#x7B97;&#x3001;&#x5E38;&#x7528;&#x6570;&#x5B66;&#x8FD0;&#x7B97;&#x7B49;&#xFF0C;&#x5728;<a href="http://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.functions$" target="_blank">DataFrame Function&#x6587;&#x6863;</a>&#x4E2D;&#x53EF;&#x4EE5;&#x67E5;&#x770B;&#x8FD9;&#x4E9B;&#x51FD;&#x6570;&#x3002;</p>
<h4 id="4&#x3001;&#x7F16;&#x7A0B;&#x6267;&#x884C;sql&#x67E5;&#x8BE2;">4&#x3001;&#x7F16;&#x7A0B;&#x6267;&#x884C;SQL&#x67E5;&#x8BE2;</h4>
<p><code>SparkSession.sql(...)</code></p>
<pre><code class="lang-scala"><span class="hljs-comment">// Register the DataFrame as a SQL temporary view</span>
df.createOrReplaceTempView(<span class="hljs-string">&quot;people&quot;</span>)

<span class="hljs-keyword">val</span> sqlDF = spark.sql(<span class="hljs-string">&quot;SELECT * FROM people&quot;</span>)
sqlDF.show()
<span class="hljs-comment">// +----+-------+</span>
<span class="hljs-comment">// | age|   name|</span>
<span class="hljs-comment">// +----+-------+</span>
<span class="hljs-comment">// |null|Michael|</span>
<span class="hljs-comment">// |  30|   Andy|</span>
<span class="hljs-comment">// |  19| Justin|</span>
<span class="hljs-comment">// +----+-------+</span>
</code></pre>
<h4 id="5&#x3001;&#x5168;&#x5C40;&#x7684;&#x4E34;&#x65F6;&#x89C6;&#x56FE;&#xFF08;global-temporary-view&#xFF09;">5&#x3001;&#x5168;&#x5C40;&#x7684;&#x4E34;&#x65F6;&#x89C6;&#x56FE;&#xFF08;Global Temporary View&#xFF09;</h4>
<p>Spark&#x4E34;&#x65F6;&#x89C6;&#x56FE;&#x7684;&#x4F5C;&#x7528;&#x57DF;&#x662F;Spark session&#xFF0C;&#x5982;&#x679C;&#x521B;&#x5EFA;&#x4E34;&#x65F6;&#x89C6;&#x56FE;&#x7684;session&#x7ED3;&#x675F;&#xFF0C;&#x4E34;&#x65F6;&#x89C6;&#x56FE;&#x5219;&#x6D88;&#x5931;&#x3002;&#x5982;&#x679C;&#x8981;&#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x6240;&#x6709;session&#x5171;&#x4EAB;&#x7684;&#x4E34;&#x65F6;&#x89C6;&#x56FE;&#xFF0C;&#x5E76;&#x4E14;&#x4F7F;&#x5B83;&#x5728;Spark&#x5E94;&#x7528;&#x7ED3;&#x675F;&#x524D;&#x4E00;&#x76F4;&#x6D3B;&#x8DC3;&#xFF0C;&#x90A3;&#x4E48;&#x53EF;&#x4EE5;&#x521B;&#x5EFA;&#x4E00;&#x4E2A;&#x5168;&#x5C40;&#x4E34;&#x65F6;&#x89C6;&#x56FE;&#x3002;&#x5168;&#x5C40;&#x4E34;&#x65F6;&#x89C6;&#x56FE;&#x88AB;&#x7ED1;&#x5B9A;&#x5230;&#x7CFB;&#x7EDF;&#x4FDD;&#x7559;&#x6570;&#x636E;&#x5E93;<code>global_temp</code>&#xFF0C;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;&#x8FD9;&#x4E2A;&#x9650;&#x5B9A;&#x540D;&#x6765;&#x4F7F;&#x7528;&#x5B83;&#x3002;</p>
<pre><code class="lang-scala"><span class="hljs-comment">// Register the DataFrame as a global temporary view</span>
df.createGlobalTempView(<span class="hljs-string">&quot;people&quot;</span>)

<span class="hljs-comment">// Global temporary view is tied to a system preserved database `global_temp`</span>
spark.sql(<span class="hljs-string">&quot;SELECT * FROM global_temp.people&quot;</span>).show()
<span class="hljs-comment">// +----+-------+</span>
<span class="hljs-comment">// | age|   name|</span>
<span class="hljs-comment">// +----+-------+</span>
<span class="hljs-comment">// |null|Michael|</span>
<span class="hljs-comment">// |  30|   Andy|</span>
<span class="hljs-comment">// |  19| Justin|</span>
<span class="hljs-comment">// +----+-------+</span>

<span class="hljs-comment">// Global temporary view is cross-session</span>
spark.newSession().sql(<span class="hljs-string">&quot;SELECT * FROM global_temp.people&quot;</span>).show()
<span class="hljs-comment">// +----+-------+</span>
<span class="hljs-comment">// | age|   name|</span>
<span class="hljs-comment">// +----+-------+</span>
<span class="hljs-comment">// |null|Michael|</span>
<span class="hljs-comment">// |  30|   Andy|</span>
<span class="hljs-comment">// |  19| Justin|</span>
<span class="hljs-comment">// +----+-------+</span>
</code></pre>
<h4 id="6&#x3001;&#x521B;&#x5EFA;datasets">6&#x3001;&#x521B;&#x5EFA;Datasets</h4>
<p>Datasets&#x4E0E;RDDs&#x7C7B;&#x4F3C;&#xFF0C;&#x4F46;&#x662F;&#x4E3A;&#x4E86;&#x5BF9;&#x5176;&#x4E2D;&#x7684;&#x5BF9;&#x8C61;&#x8FDB;&#x884C;&#x5904;&#x7406;&#x6216;&#x8005;&#x901A;&#x8FC7;&#x7F51;&#x7EDC;&#x4F20;&#x8F93;&#xFF0C;&#x9700;&#x8981;&#x4F7F;&#x7528;<code>Encoder</code>&#x5BF9;&#x5BF9;&#x8C61;&#x8FDB;&#x884C;&#x5E8F;&#x5217;&#x5316;&#xFF0C;&#x800C;&#x4E0D;&#x662F;&#x4F7F;&#x7528;Java&#x5E8F;&#x5217;&#x5316;&#x6216;&#x8005;Kryo&#x3002;&#x7136;&#x800C;&#xFF0C;&#x8981;&#x628A;&#x5BF9;&#x8C61;&#x8F6C;&#x6362;&#x4E3A;&#x5B57;&#x8282;encoders&#x548C;&#x6807;&#x51C6;&#x7684;&#x5E8F;&#x5217;&#x5316;&#x90FD;&#x662F;&#x9700;&#x8981;&#x7684;&#xFF0C;encoders&#x662F;&#x52A8;&#x6001;&#x5730;&#x751F;&#x6210;&#x7684;&#x4EE3;&#x7801;&#x5E76;&#x4E14;&#x4F7F;&#x7528;&#x4E00;&#x79CD;&#x683C;&#x5F0F;&#xFF0C;&#x8BA9;Spark&#x53EF;&#x4EE5;&#x5728;&#x4E0D;&#x628A;&#x5B57;&#x8282;&#x53CD;&#x5E8F;&#x5217;&#x5316;&#x56DE;&#x5BF9;&#x8C61;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x6267;&#x884C;&#x8BB8;&#x591A;&#x50CF;&#x8FC7;&#x6EE4;&#x3001;&#x6392;&#x5E8F;&#x548C;&#x54C8;&#x5E0C;&#x7B49;&#x8BB8;&#x591A;&#x64CD;&#x4F5C;&#x3002;</p>
<pre><code class="lang-scala"><span class="hljs-comment">// Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,</span>
<span class="hljs-comment">// you can use custom classes that implement the Product interface</span>
<span class="hljs-keyword">case</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span>(<span class="hljs-params">name: <span class="hljs-type">String</span>, age: <span class="hljs-type">Long</span></span>)</span>

<span class="hljs-comment">// Encoders are created for case classes</span>
<span class="hljs-keyword">val</span> caseClassDS = <span class="hljs-type">Seq</span>(<span class="hljs-type">Person</span>(<span class="hljs-string">&quot;Andy&quot;</span>, <span class="hljs-number">32</span>)).toDS()
caseClassDS.show()
<span class="hljs-comment">// +----+---+</span>
<span class="hljs-comment">// |name|age|</span>
<span class="hljs-comment">// +----+---+</span>
<span class="hljs-comment">// |Andy| 32|</span>
<span class="hljs-comment">// +----+---+</span>

<span class="hljs-comment">// Encoders for most common types are automatically provided by importing spark.implicits._</span>
<span class="hljs-keyword">val</span> primitiveDS = <span class="hljs-type">Seq</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>).toDS()
primitiveDS.map(_ + <span class="hljs-number">1</span>).collect() <span class="hljs-comment">// Returns: Array(2, 3, 4)</span>

<span class="hljs-comment">// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name</span>
<span class="hljs-keyword">val</span> path = <span class="hljs-string">&quot;examples/src/main/resources/people.json&quot;</span>
<span class="hljs-keyword">val</span> peopleDS = spark.read.json(path).as[<span class="hljs-type">Person</span>]
peopleDS.show()
<span class="hljs-comment">// +----+-------+</span>
<span class="hljs-comment">// | age|   name|</span>
<span class="hljs-comment">// +----+-------+</span>
<span class="hljs-comment">// |null|Michael|</span>
<span class="hljs-comment">// |  30|   Andy|</span>
<span class="hljs-comment">// |  19| Justin|</span>
<span class="hljs-comment">// +----+-------+</span>
</code></pre>
<h4 id="7&#x3001;&#x4E0E;rdds&#x7684;&#x4E92;&#x64CD;&#x4F5C;&#xFF08;interoperating-with-rdds&#xFF09;">7&#x3001;&#x4E0E;RDDs&#x7684;&#x4E92;&#x64CD;&#x4F5C;&#xFF08;interoperating with RDDs&#xFF09;</h4>
<p>Spark SQL&#x652F;&#x6301;&#x4E24;&#x79CD;&#x628A;&#x5B58;&#x5728;&#x7684;RDDs&#x8F6C;&#x6362;&#x4E3A;Datasets&#x7684;&#x65B9;&#x6CD5;&#xFF1A;&#x7B2C;&#x4E00;&#xFF0C;&#x4F7F;&#x7528;&#x53CD;&#x5C04;&#x6765;&#x63A8;&#x6D4B;&#x5305;&#x542B;&#x7279;&#x5B9A;&#x7C7B;&#x578B;&#x5BF9;&#x8C61;&#x7684;RDD&#x7684;schema&#xFF1B;&#x5DF2;&#x7ECF;&#x77E5;&#x9053;schema&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x5728;&#x7F16;&#x5199;Spark&#x5E94;&#x7528;&#x65F6;&#xFF0C;&#x8FD9;&#x79CD;&#x57FA;&#x4E8E;&#x53CD;&#x5C04;&#x7684;&#x65B9;&#x6CD5;&#x4F7F;&#x4EE3;&#x7801;&#x66F4;&#x52A0;&#x7B80;&#x6D01;&#x3002;</p>
<p>&#x7B2C;&#x4E8C;&#x79CD;&#x65B9;&#x6CD5;&#xFF0C;&#x901A;&#x8FC7;&#x7A0B;&#x5F0F;&#x5316;&#x7684;&#x7ED3;&#x6784;&#x521B;&#x5EFA;schema&#x5E76;&#x5E94;&#x7528;&#x5230;&#x5B58;&#x5728;&#x7684;RDD&#x4E0A;&#x3002;&#x5C3D;&#x7BA1;&#x8FD9;&#x79CD;&#x65B9;&#x6CD5;&#x66F4;&#x52A0;&#x590D;&#x6742;&#xFF0C;&#x4F46;&#x662F;&#xFF0C;&#x5728;&#x53EA;&#x6709;&#x8FD0;&#x884C;&#x65F6;&#x624D;&#x80FD;&#x77E5;&#x9053;&#x6570;&#x636E;&#x7C7B;&#x578B;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x4F7F;&#x5F97;&#x6784;&#x5EFA;Datasets&#x6210;&#x4E3A;&#x53EF;&#x80FD;&#x3002;</p>
<h5 id="71&#x3001;&#x4F7F;&#x7528;&#x53CD;&#x5C04;&#x63A8;&#x6D4B;schema">7.1&#x3001;&#x4F7F;&#x7528;&#x53CD;&#x5C04;&#x63A8;&#x6D4B;Schema</h5>
<p>Spark SQL&#x7684;Scala&#x63A5;&#x53E3;&#x652F;&#x6301;&#x81EA;&#x52A8;&#x5730;&#x5C06;&#x5305;&#x542B;case classes&#x7684;RDD&#x8F6C;&#x6362;&#x4E3A;DataFrame&#x3002;case class&#x5B9A;&#x4E49;&#x4E86;&#x8868;&#x7684;schema&#x3002;&#x4F7F;&#x7528;&#x53CD;&#x5C04;&#x8BFB;&#x53D6;case class&#x7684;&#x53C2;&#x6570;&#x540D;&#x5E76;&#x4F5C;&#x4E3A;&#x5217;&#x540D;&#x3002;case classes&#x53EF;&#x4EE5;&#x5D4C;&#x5957;&#x6216;&#x8005;&#x5305;&#x542B;&#x50CF;<code>Seq</code>&#x6216;&#x8005;<code>Array</code>&#x8FD9;&#x6837;&#x7684;&#x6577;&#x5728;&#x7C7B;&#x578B;&#x3002;&#x8FD9;&#x79CD;RDD&#x53EF;&#x4EE5;&#x9690;&#x5F0F;&#x5F97;&#x8F6C;&#x6362;&#x4E3A;DataFrame&#x5E76;&#x4E14;&#x7136;&#x540E;&#x88AB;&#x6CE8;&#x518C;&#x4E3A;&#x4E00;&#x4E2A;&#x8868;&#x3002;&#x8868;&#x53EF;&#x4EE5;&#x7528;&#x5728;&#x540E;&#x7EED;&#x7684;SQL&#x8BED;&#x53E5;&#x4E2D;&#x3002;</p>
<pre><code class="lang-scala">/ <span class="hljs-type">For</span> <span class="hljs-keyword">implicit</span> conversions from <span class="hljs-type">RDDs</span> to <span class="hljs-type">DataFrames</span>
<span class="hljs-keyword">import</span> spark.implicits._

<span class="hljs-comment">// Create an RDD of Person objects from a text file, convert it to a Dataframe</span>
<span class="hljs-keyword">val</span> peopleDF = spark.sparkContext
  .textFile(<span class="hljs-string">&quot;examples/src/main/resources/people.txt&quot;</span>)
  .map(_.split(<span class="hljs-string">&quot;,&quot;</span>))
  .map(attributes =&gt; <span class="hljs-type">Person</span>(attributes(<span class="hljs-number">0</span>), attributes(<span class="hljs-number">1</span>).trim.toInt))
  .toDF()
<span class="hljs-comment">// Register the DataFrame as a temporary view</span>
peopleDF.createOrReplaceTempView(<span class="hljs-string">&quot;people&quot;</span>)

<span class="hljs-comment">// SQL statements can be run by using the sql methods provided by Spark</span>
<span class="hljs-keyword">val</span> teenagersDF = spark.sql(<span class="hljs-string">&quot;SELECT name, age FROM people WHERE age BETWEEN 13 AND 19&quot;</span>)

<span class="hljs-comment">// The columns of a row in the result can be accessed by field index</span>
teenagersDF.map(teenager =&gt; <span class="hljs-string">&quot;Name: &quot;</span> + teenager(<span class="hljs-number">0</span>)).show()
<span class="hljs-comment">// +------------+</span>
<span class="hljs-comment">// |       value|</span>
<span class="hljs-comment">// +------------+</span>
<span class="hljs-comment">// |Name: Justin|</span>
<span class="hljs-comment">// +------------+</span>

<span class="hljs-comment">// or by field name</span>
teenagersDF.map(teenager =&gt; <span class="hljs-string">&quot;Name: &quot;</span> + teenager.getAs[<span class="hljs-type">String</span>](<span class="hljs-string">&quot;name&quot;</span>)).show()
<span class="hljs-comment">// +------------+</span>
<span class="hljs-comment">// |       value|</span>
<span class="hljs-comment">// +------------+</span>
<span class="hljs-comment">// |Name: Justin|</span>
<span class="hljs-comment">// +------------+</span>

<span class="hljs-comment">// No pre-defined encoders for Dataset[Map[K,V]], define explicitly</span>
<span class="hljs-keyword">implicit</span> <span class="hljs-keyword">val</span> mapEncoder = org.apache.spark.sql.<span class="hljs-type">Encoders</span>.kryo[<span class="hljs-type">Map</span>[<span class="hljs-type">String</span>, <span class="hljs-type">Any</span>]]
<span class="hljs-comment">// Primitive types and case classes can be also defined as</span>
<span class="hljs-comment">// implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()</span>

<span class="hljs-comment">// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]</span>
teenagersDF.map(teenager =&gt; teenager.getValuesMap[<span class="hljs-type">Any</span>](<span class="hljs-type">List</span>(<span class="hljs-string">&quot;name&quot;</span>, <span class="hljs-string">&quot;age&quot;</span>))).collect()
<span class="hljs-comment">// Array(Map(&quot;name&quot; -&gt; &quot;Justin&quot;, &quot;age&quot; -&gt; 19))</span>
</code></pre>
<h5 id="72&#x3001;&#x7A0B;&#x5F0F;&#x5316;&#x5730;&#x6307;&#x5B9A;schema">7.2&#x3001;&#x7A0B;&#x5F0F;&#x5316;&#x5730;&#x6307;&#x5B9A;Schema</h5>
<p>&#x5982;&#x679C;&#x4E0D;&#x80FD;&#x63D0;&#x524D;&#x5B9A;&#x4E49;case classes&#xFF08;&#x6BD4;&#x5982;&#xFF0C;&#x8BB0;&#x5F55;&#x7684;&#x7ED3;&#x6784;&#x7F16;&#x7801;&#x5728;&#x4E00;&#x4E2A;&#x5B57;&#x7B26;&#x4E32;&#x4E2D;&#xFF0C;&#x6216;&#x8005;&#x8981;&#x89E3;&#x6790;&#x6587;&#x672C;&#x6570;&#x636E;&#x96C6;&#xFF0C;&#x5E76;&#x4E14;&#x4E0D;&#x540C;&#x7684;&#x7528;&#x6237;&#x5BF9;&#x5E94;&#x4E0D;&#x540C;&#x7684;&#x5B57;&#x6BB5;<em>for example&#xFF0C; the structure of records is encoded in a string&#xFF0C;or a text dataset will be parsed and fields will be projected differently for different users</em>&#xFF09;&#xFF0C;&#x901A;&#x8FC7;&#x4EE5;&#x4E0B;&#x4E09;&#x6B65;&#x53EF;&#x4EE5;&#x7A0B;&#x5F0F;&#x5316;&#x5730;&#x521B;&#x5EFA;<code>DataFrame</code>&#xFF1A;</p>
<ol>
<li>&#x4ECE;&#x539F;&#x59CB;RDD&#x521B;&#x5EFA;&#x4E00;&#x4E2A;<code>Row</code>s&#x7684;RDD&#xFF1B;</li>
<li>&#x521B;&#x5EFA;&#x7528;<code>StructType</code>&#x8868;&#x793A;&#x7B2C;&#x4E00;&#x6B65;&#x521B;&#x5EFA;&#x7684;RDD&#x7684;<code>Row</code>s&#x7684;&#x7ED3;&#x6784;&#x5BF9;&#x5E94;&#x7684;schema&#xFF1B;</li>
<li>&#x901A;&#x8FC7;<code>SparkSession</code>&#x63D0;&#x4F9B;&#x7684;<code>createDataFrame</code>&#x65B9;&#x6CD5;&#x628A;schema&#x5E94;&#x7528;&#x5230;RDD&#x3002;</li>
</ol>
<pre><code class="lang-scala"><span class="hljs-keyword">import</span> org.apache.spark.sql.types._

<span class="hljs-comment">// Create an RDD</span>
<span class="hljs-keyword">val</span> peopleRDD = spark.sparkContext.textFile(<span class="hljs-string">&quot;examples/src/main/resources/people.txt&quot;</span>)

<span class="hljs-comment">// The schema is encoded in a string</span>
<span class="hljs-keyword">val</span> schemaString = <span class="hljs-string">&quot;name age&quot;</span>

<span class="hljs-comment">// Generate the schema based on the string of schema</span>
<span class="hljs-keyword">val</span> fields = schemaString.split(<span class="hljs-string">&quot; &quot;</span>)
  .map(fieldName =&gt; <span class="hljs-type">StructField</span>(fieldName, <span class="hljs-type">StringType</span>, nullable = <span class="hljs-literal">true</span>))
<span class="hljs-keyword">val</span> schema = <span class="hljs-type">StructType</span>(fields)

<span class="hljs-comment">// Convert records of the RDD (people) to Rows</span>
<span class="hljs-keyword">val</span> rowRDD = peopleRDD
  .map(_.split(<span class="hljs-string">&quot;,&quot;</span>))
  .map(attributes =&gt; <span class="hljs-type">Row</span>(attributes(<span class="hljs-number">0</span>), attributes(<span class="hljs-number">1</span>).trim))

<span class="hljs-comment">// Apply the schema to the RDD</span>
<span class="hljs-keyword">val</span> peopleDF = spark.createDataFrame(rowRDD, schema)

<span class="hljs-comment">// Creates a temporary view using the DataFrame</span>
peopleDF.createOrReplaceTempView(<span class="hljs-string">&quot;people&quot;</span>)

<span class="hljs-comment">// SQL can be run over a temporary view created using DataFrames</span>
<span class="hljs-keyword">val</span> results = spark.sql(<span class="hljs-string">&quot;SELECT name FROM people&quot;</span>)

<span class="hljs-comment">// The results of SQL queries are DataFrames and support all the normal RDD operations</span>
<span class="hljs-comment">// The columns of a row in the result can be accessed by field index or by field name</span>
results.map(attributes =&gt; <span class="hljs-string">&quot;Name: &quot;</span> + attributes(<span class="hljs-number">0</span>)).show()
<span class="hljs-comment">// +-------------+</span>
<span class="hljs-comment">// |        value|</span>
<span class="hljs-comment">// +-------------+</span>
<span class="hljs-comment">// |Name: Michael|</span>
<span class="hljs-comment">// |   Name: Andy|</span>
<span class="hljs-comment">// | Name: Justin|</span>
<span class="hljs-comment">// +-------------+</span>
</code></pre>
<h4 id="8&#x3001;&#x805A;&#x5408;">8&#x3001;&#x805A;&#x5408;</h4>
<p><a href="http://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.functions$" target="_blank">&#x5185;&#x7F6E;&#x7684;DataFrame&#x51FD;&#x6570;</a>&#x63D0;&#x4F9B;&#x4E86;&#x50CF; <code>count()</code>, <code>countDistinct()</code>, <code>avg()</code>, <code>max()</code>, <code>min()</code>, &#x7B49;&#x7B49;&#x7684;&#x901A;&#x7528;&#x805A;&#x5408;&#x51FD;&#x6570;&#x3002;&#x4F46;&#x662F;&#x8FD9;&#x4E9B;&#x51FD;&#x6570;&#x662F;&#x4E3A;DataFrames&#x8BBE;&#x8BA1;&#x7684;&#xFF0C;Spark SQL&#x7684;Scala&#x548C;Java API&#x8FD8;&#x6709;&#x4E00;&#x4E9B;&#x7C7B;&#x578B;&#x5B89;&#x5168;&#xFF08;type-safe&#xFF09;&#x7248;&#x672C;&#x5185;&#x7F6E;&#x51FD;&#x6570;&#x53EF;&#x4EE5;&#x7528;&#x4E8E;&#x5F3A;&#x7C7B;&#x578B;&#x7684;Datasets&#x3002;&#x53E6;&#x5916;&#xFF0C;&#x9664;&#x4E86;&#x9884;&#x5B9A;&#x4E49;&#x7684;&#x805A;&#x5408;&#x51FD;&#x6570;&#xFF0C;&#x7528;&#x6237;&#x8FD8;&#x53EF;&#x4EE5;&#x81EA;&#x5B9A;&#x4E49;&#x805A;&#x5408;&#x51FD;&#x6570;&#x3002;</p>
<h5 id="81&#x3001;&#x65E0;&#x7C7B;&#x578B;&#x7528;&#x6237;&#x5B9A;&#x4E49;&#x805A;&#x5408;&#x51FD;&#x6570;&#xFF08;untyped-user-defined-aggregate-functions&#xFF09;">8.1&#x3001;&#x65E0;&#x7C7B;&#x578B;&#x7528;&#x6237;&#x5B9A;&#x4E49;&#x805A;&#x5408;&#x51FD;&#x6570;&#xFF08;Untyped User-Defined Aggregate Functions&#xFF09;</h5>
<p>&#x7528;&#x6237;&#x5FC5;&#x987B;&#x7EE7;&#x627F;<a href="http://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.expressions.UserDefinedAggregateFunction" target="_blank">UserDefinedAggregateFunction</a>&#x8FD9;&#x4E2A;&#x62BD;&#x8C61;&#x7C7B;&#x6765;&#x5B9E;&#x73B0;&#x4E00;&#x4E2A;&#x81EA;&#x5B9A;&#x4E49;&#x7684;&#x65E0;&#x7C7B;&#x578B;&#x805A;&#x5408;&#x51FD;&#x6570;&#x3002;&#x6BD4;&#x5982;&#xFF0C;&#x53EF;&#x4EE5;&#x5B9A;&#x4E49;&#x4E00;&#x4E2A;&#x6C42;&#x5E73;&#x5747;&#x503C;&#x5F97;&#x51FD;&#x6570;&#x5982;&#x4E0B;&#xFF1A;</p>
<pre><code class="lang-scala"><span class="hljs-keyword">import</span> org.apache.spark.sql.expressions.<span class="hljs-type">MutableAggregationBuffer</span>
<span class="hljs-keyword">import</span> org.apache.spark.sql.expressions.<span class="hljs-type">UserDefinedAggregateFunction</span>
<span class="hljs-keyword">import</span> org.apache.spark.sql.types._
<span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">Row</span>
<span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">SparkSession</span>

<span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">MyAverage</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">UserDefinedAggregateFunction</span> </span>{
  <span class="hljs-comment">// Data types of input arguments of this aggregate function</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">inputSchema</span></span>: <span class="hljs-type">StructType</span> = <span class="hljs-type">StructType</span>(<span class="hljs-type">StructField</span>(<span class="hljs-string">&quot;inputColumn&quot;</span>, <span class="hljs-type">LongType</span>) :: <span class="hljs-type">Nil</span>)
  <span class="hljs-comment">// Data types of values in the aggregation buffer</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bufferSchema</span></span>: <span class="hljs-type">StructType</span> = {
    <span class="hljs-type">StructType</span>(<span class="hljs-type">StructField</span>(<span class="hljs-string">&quot;sum&quot;</span>, <span class="hljs-type">LongType</span>) :: <span class="hljs-type">StructField</span>(<span class="hljs-string">&quot;count&quot;</span>, <span class="hljs-type">LongType</span>) :: <span class="hljs-type">Nil</span>)
  }
  <span class="hljs-comment">// The data type of the returned value</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dataType</span></span>: <span class="hljs-type">DataType</span> = <span class="hljs-type">DoubleType</span>
  <span class="hljs-comment">// Whether this function always returns the same output on the identical input</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">deterministic</span></span>: <span class="hljs-type">Boolean</span> = <span class="hljs-literal">true</span>
  <span class="hljs-comment">// Initializes the given aggregation buffer. The buffer itself is a `Row` that in addition to</span>
  <span class="hljs-comment">// standard methods like retrieving a value at an index (e.g., get(), getBoolean()), provides</span>
  <span class="hljs-comment">// the opportunity to update its values. Note that arrays and maps inside the buffer are still</span>
  <span class="hljs-comment">// immutable.</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">initialize</span></span>(buffer: <span class="hljs-type">MutableAggregationBuffer</span>): <span class="hljs-type">Unit</span> = {
    buffer(<span class="hljs-number">0</span>) = <span class="hljs-number">0</span>L
    buffer(<span class="hljs-number">1</span>) = <span class="hljs-number">0</span>L
  }
  <span class="hljs-comment">// Updates the given aggregation buffer `buffer` with new input data from `input`</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update</span></span>(buffer: <span class="hljs-type">MutableAggregationBuffer</span>, input: <span class="hljs-type">Row</span>): <span class="hljs-type">Unit</span> = {
    <span class="hljs-keyword">if</span> (!input.isNullAt(<span class="hljs-number">0</span>)) {
      buffer(<span class="hljs-number">0</span>) = buffer.getLong(<span class="hljs-number">0</span>) + input.getLong(<span class="hljs-number">0</span>)
      buffer(<span class="hljs-number">1</span>) = buffer.getLong(<span class="hljs-number">1</span>) + <span class="hljs-number">1</span>
    }
  }
  <span class="hljs-comment">// Merges two aggregation buffers and stores the updated buffer values back to `buffer1`</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">merge</span></span>(buffer1: <span class="hljs-type">MutableAggregationBuffer</span>, buffer2: <span class="hljs-type">Row</span>): <span class="hljs-type">Unit</span> = {
    buffer1(<span class="hljs-number">0</span>) = buffer1.getLong(<span class="hljs-number">0</span>) + buffer2.getLong(<span class="hljs-number">0</span>)
    buffer1(<span class="hljs-number">1</span>) = buffer1.getLong(<span class="hljs-number">1</span>) + buffer2.getLong(<span class="hljs-number">1</span>)
  }
  <span class="hljs-comment">// Calculates the final result</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluate</span></span>(buffer: <span class="hljs-type">Row</span>): <span class="hljs-type">Double</span> = buffer.getLong(<span class="hljs-number">0</span>).toDouble / buffer.getLong(<span class="hljs-number">1</span>)
}

<span class="hljs-comment">// Register the function to access it</span>
spark.udf.register(<span class="hljs-string">&quot;myAverage&quot;</span>, <span class="hljs-type">MyAverage</span>)

<span class="hljs-keyword">val</span> df = spark.read.json(<span class="hljs-string">&quot;examples/src/main/resources/employees.json&quot;</span>)
df.createOrReplaceTempView(<span class="hljs-string">&quot;employees&quot;</span>)
df.show()
<span class="hljs-comment">// +-------+------+</span>
<span class="hljs-comment">// |   name|salary|</span>
<span class="hljs-comment">// +-------+------+</span>
<span class="hljs-comment">// |Michael|  3000|</span>
<span class="hljs-comment">// |   Andy|  4500|</span>
<span class="hljs-comment">// | Justin|  3500|</span>
<span class="hljs-comment">// |  Berta|  4000|</span>
<span class="hljs-comment">// +-------+------+</span>

<span class="hljs-keyword">val</span> result = spark.sql(<span class="hljs-string">&quot;SELECT myAverage(salary) as average_salary FROM employees&quot;</span>)
result.show()
<span class="hljs-comment">// +--------------+</span>
<span class="hljs-comment">// |average_salary|</span>
<span class="hljs-comment">// +--------------+</span>
<span class="hljs-comment">// |        3750.0|</span>
<span class="hljs-comment">// +--------------+</span>
</code></pre>
<h5 id="82&#x3001;&#x7C7B;&#x578B;&#x5B89;&#x5168;&#x7684;&#x7528;&#x6237;&#x5B9A;&#x4E49;&#x805A;&#x5408;&#x51FD;&#x6570;&#xFF08;type-safe-user-defined-aggregate-functions&#xFF09;">8.2&#x3001;&#x7C7B;&#x578B;&#x5B89;&#x5168;&#x7684;&#x7528;&#x6237;&#x5B9A;&#x4E49;&#x805A;&#x5408;&#x51FD;&#x6570;&#xFF08;Type-Safe User-Defined Aggregate Functions&#xFF09;</h5>
<p>&#x5F3A;&#x7C7B;&#x578B;Datasets&#x7684;&#x7528;&#x6237;&#x5B9A;&#x4E49;&#x805A;&#x5408;&#x51FD;&#x6570;&#x8981;&#x7EE7;&#x627F;<a href="http://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.expressions.Aggregator" target="_blank">Aggregator</a>&#x62BD;&#x8C61;&#x7C7B;&#x3002;&#x6BD4;&#x5982;&#xFF0C;&#x53EF;&#x4EE5;&#x5B9A;&#x4E49;&#x4E00;&#x4E2A;&#x7C7B;&#x578B;&#x5B89;&#x5168;&#x7684;&#x7528;&#x6237;&#x5B9A;&#x4E49;&#x6C42;&#x5E73;&#x5747;&#x503C;&#x5F97;&#x51FD;&#x6570;&#x5982;&#x4E0B;&#xFF1A;</p>
<pre><code class="lang-scala"><span class="hljs-keyword">import</span> org.apache.spark.sql.expressions.<span class="hljs-type">Aggregator</span>
<span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">Encoder</span>
<span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">Encoders</span>
<span class="hljs-keyword">import</span> org.apache.spark.sql.<span class="hljs-type">SparkSession</span>

<span class="hljs-keyword">case</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Employee</span>(<span class="hljs-params">name: <span class="hljs-type">String</span>, salary: <span class="hljs-type">Long</span></span>)</span>
<span class="hljs-keyword">case</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Average</span>(<span class="hljs-params">var sum: <span class="hljs-type">Long</span>, var count: <span class="hljs-type">Long</span></span>)</span>

<span class="hljs-class"><span class="hljs-keyword">object</span> <span class="hljs-title">MyAverage</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Aggregator</span>[<span class="hljs-type">Employee</span>, <span class="hljs-type">Average</span>, <span class="hljs-type">Double</span>] </span>{
  <span class="hljs-comment">// A zero value for this aggregation. Should satisfy the property that any b + zero = b</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">zero</span></span>: <span class="hljs-type">Average</span> = <span class="hljs-type">Average</span>(<span class="hljs-number">0</span>L, <span class="hljs-number">0</span>L)
  <span class="hljs-comment">// Combine two values to produce a new value. For performance, the function may modify `buffer`</span>
  <span class="hljs-comment">// and return it instead of constructing a new object</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reduce</span></span>(buffer: <span class="hljs-type">Average</span>, employee: <span class="hljs-type">Employee</span>): <span class="hljs-type">Average</span> = {
    buffer.sum += employee.salary
    buffer.count += <span class="hljs-number">1</span>
    buffer
  }
  <span class="hljs-comment">// Merge two intermediate values</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">merge</span></span>(b1: <span class="hljs-type">Average</span>, b2: <span class="hljs-type">Average</span>): <span class="hljs-type">Average</span> = {
    b1.sum += b2.sum
    b1.count += b2.count
    b1
  }
  <span class="hljs-comment">// Transform the output of the reduction</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">finish</span></span>(reduction: <span class="hljs-type">Average</span>): <span class="hljs-type">Double</span> = reduction.sum.toDouble / reduction.count
  <span class="hljs-comment">// Specifies the Encoder for the intermediate value type</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">bufferEncoder</span></span>: <span class="hljs-type">Encoder</span>[<span class="hljs-type">Average</span>] = <span class="hljs-type">Encoders</span>.product
  <span class="hljs-comment">// Specifies the Encoder for the final output value type</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">outputEncoder</span></span>: <span class="hljs-type">Encoder</span>[<span class="hljs-type">Double</span>] = <span class="hljs-type">Encoders</span>.scalaDouble
}

<span class="hljs-keyword">val</span> ds = spark.read.json(<span class="hljs-string">&quot;examples/src/main/resources/employees.json&quot;</span>).as[<span class="hljs-type">Employee</span>]
ds.show()
<span class="hljs-comment">// +-------+------+</span>
<span class="hljs-comment">// |   name|salary|</span>
<span class="hljs-comment">// +-------+------+</span>
<span class="hljs-comment">// |Michael|  3000|</span>
<span class="hljs-comment">// |   Andy|  4500|</span>
<span class="hljs-comment">// | Justin|  3500|</span>
<span class="hljs-comment">// |  Berta|  4000|</span>
<span class="hljs-comment">// +-------+------+</span>

<span class="hljs-comment">// Convert the function to a `TypedColumn` and give it a name</span>
<span class="hljs-keyword">val</span> averageSalary = <span class="hljs-type">MyAverage</span>.toColumn.name(<span class="hljs-string">&quot;average_salary&quot;</span>)
<span class="hljs-keyword">val</span> result = ds.select(averageSalary)
result.show()
<span class="hljs-comment">// +--------------+</span>
<span class="hljs-comment">// |average_salary|</span>
<span class="hljs-comment">// +--------------+</span>
<span class="hljs-comment">// |        3750.0|</span>
<span class="hljs-comment">// +--------------+</span>
</code></pre>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="../spark-sql.html" class="navigation navigation-prev " aria-label="Previous page: Spark SQL">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="shu-ju-yuan.html" class="navigation navigation-next " aria-label="Next page: 数据源">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Getting started","level":"1.4.1","depth":2,"next":{"title":"数据源","level":"1.4.2","depth":2,"path":"spark-sql/shu-ju-yuan.md","ref":"spark-sql/shu-ju-yuan.md","articles":[]},"previous":{"title":"Spark SQL","level":"1.4","depth":1,"path":"spark-sql.md","ref":"spark-sql.md","articles":[{"title":"Getting started","level":"1.4.1","depth":2,"path":"spark-sql/getting-started.md","ref":"spark-sql/getting-started.md","articles":[]},{"title":"数据源","level":"1.4.2","depth":2,"path":"spark-sql/shu-ju-yuan.md","ref":"spark-sql/shu-ju-yuan.md","articles":[]},{"title":"性能调试","level":"1.4.3","depth":2,"path":"spark-sql/xing-neng-diao-shi.md","ref":"spark-sql/xing-neng-diao-shi.md","articles":[]},{"title":"分布式SQL引擎","level":"1.4.4","depth":2,"path":"spark-sql/fen-bu-shi-sql-yin-qing.md","ref":"spark-sql/fen-bu-shi-sql-yin-qing.md","articles":[]},{"title":"与Apache Hive的兼容性","level":"1.4.5","depth":2,"path":"spark-sql/yu-apache-hive-de-jian-rong-xing.md","ref":"spark-sql/yu-apache-hive-de-jian-rong-xing.md","articles":[]},{"title":"数据类型","level":"1.4.6","depth":2,"path":"spark-sql/shu-ju-lei-xing.md","ref":"spark-sql/shu-ju-lei-xing.md","articles":[]},{"title":"内置函数","level":"1.4.7","depth":2,"path":"spark-sql/built-in-functions.md","ref":"spark-sql/built-in-functions.md","articles":[]}]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":["katex"],"pluginsConfig":{"katex":{},"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"spark-sql/getting-started.md","mtime":"2020-12-30T08:41:57.580Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2021-02-04T09:47:13.100Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

