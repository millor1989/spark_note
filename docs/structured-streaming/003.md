### Structured Streaming集成Kafka

支持Kafka broker的版本为0.10.0或以上

#### 依赖

对于使用SBT的Scala/Java应用：

```
groupId = org.apache.spark
artifactId = spark-sql-kafka-0-10_2.11
version = 2.2.0
```

#### 1、从Kafka读取数据

##### 1.1、创建一个用于流查询的Kafka数据源

```scala
// Subscribe to 1 topic
val df = spark
  .readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .option("subscribe", "topic1")
  .load()
df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
  .as[(String, String)]

// Subscribe to multiple topics
val df = spark
  .readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .option("subscribe", "topic1,topic2")
  .load()
df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
  .as[(String, String)]

// Subscribe to a pattern
val df = spark
  .readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .option("subscribePattern", "topic.*")
  .load()
df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
  .as[(String, String)]
```

##### 1.2、创建一个用于批查询的Kafka数据源

```scala
// Subscribe to 1 topic defaults to the earliest and latest offsets
val df = spark
  .read
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .option("subscribe", "topic1")
  .load()
df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
  .as[(String, String)]

// Subscribe to multiple topics, specifying explicit Kafka offsets
val df = spark
  .read
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .option("subscribe", "topic1,topic2")
  .option("startingOffsets", """{"topic1":{"0":23,"1":-2},"topic2":{"0":-2}}""")
  .option("endingOffsets", """{"topic1":{"0":50,"1":-1},"topic2":{"0":-1}}""")
  .load()
df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
  .as[(String, String)]

// Subscribe to a pattern, at the earliest and latest offsets
val df = spark
  .read
  .format("kafka")
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
  .option("subscribePattern", "topic.*")
  .option("startingOffsets", "earliest")
  .option("endingOffsets", "latest")
  .load()
df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
  .as[(String, String)]
```

数据源中每一行的schema为：

| 字段          | 类型   |
| ------------- | ------ |
| key           | binary |
| value         | binary |
| topic         | string |
| partition     | int    |
| offset        | long   |
| timestamp     | long   |
| timestampType | int    |

对于Kafka数据源，不管是批查询还是流查询都必须指定如下选项：

| Option                  | value                                     | 含义                                                         |
| ----------------------- | ----------------------------------------- | ------------------------------------------------------------ |
| assign                  | json字符串{"topicA":[0,1],"topicB":[2,4]} | 指定消费的主题分区。对于Kafka数据源，“assign”，“subscribe”或“subscribePattern”三个选项只能指定一个。 |
| subscribe               | 逗号分隔得主题集合                        | 要订阅的主题集合。对于Kafka数据源，“assign”，“subscribe”或“subscribePattern”三个选项只能指定一个。 |
| subscribePattern        | Java 正则字符串                           | 用于订阅主题的模式。对于Kafka数据源，“assign”，“subscribe”或“subscribePattern”三个选项只能指定一个。 |
| kafka.bootstrap.servers | 逗号分隔的`host:port`集合                 | Kafka “bootstrap.servers”配置                                |

如下选项是可选的：

| Option                      | value                                                        | default                          | 查询类型 | 含义                                                         |
| --------------------------- | ------------------------------------------------------------ | -------------------------------- | -------- | ------------------------------------------------------------ |
| startingOffsets             | “earliest”，“latest”（仅流适用），或者json字符串{"topicA":{"0":23,"1":-1},"topicB":{"0":-2}} | 流查询“latest”，批查询“earliest” | 流和批   | 启动查询时的开始位置，可以是“earliest”从最早的偏移开始，“latest”从最新的偏移开始，或者指定了每个主题分区起始偏移的json字符串。在json中，-2用作偏移指的是最早的偏移，-1指的是最新的偏移。注意：对于批查询，不能使用最新的偏移。对于流查询，当新的查询启动时这个设置才会应用，重新开始（resuming）总是从查询离开（left off）的地方继续。查询过程中新发现的分区将从最早的开始。 |
| endingOffset                | “latest”或者json字符串{"topicA":{"0":23,"1":-1},"topicB":{"0":-1}} | “latest”                         | 批查询   | 批查询的结束点。“latest”指的是最新的，json字符串指定每个主题分区的结束偏移。在json中，-1是指最新的，不能使用-2（最早的） |
| failOnDataLoss              | true或false                                                  | true                             | 流查询   | 当有可能数据丢失了时（比如，主题被删除，或者偏移超出范围），将查询失败。这可能是一个false报警。可以在不符合期望时关闭它。对于批查询，如果因为数据丢失它不能从指定的偏移读取到数据，它将会失败。 |
| kafkaConsumer.pollTimeoutMs | long                                                         | 512                              | 流或批   | executors从Kafka poll数据的超时时间，毫秒                    |
| fetchOffset.numRetries      | int                                                          | 3                                | 流或批   | 获取Kafka 偏移的重试次数                                     |
| fetchOffset.retryIntervalMs | long                                                         | 10                               | 流或批   | 重试获取Kafka偏移前等待的时间，毫秒                          |
| maxOffsetsPerTrigger        | long                                                         | none                             | 流或批   | 每个触发间隔处理的最大偏移数量的速率。指定的偏移总数将在不同容量的主题分区之间按比例分配 |

#### 2、往Kafka写数据