## Broadcasting Large Variables

Using the [broadcast functionality](https://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#broadcast-variables) available in `SparkContext` can greatly reduce the size of each serialized task, and the cost of launching a job over a cluster. If your tasks use any large object from the driver program inside of them (e.g. a static lookup table), consider turning it into a broadcast variable. Spark prints the serialized size of each task on the master, so you can look at that to decide whether your tasks are too large; in general tasks larger than about 20 KB are probably worth optimizing.

官方文档中广播大变量的描述。没看太懂……

Spark prints the serialized size of each task on the master。task的序列化以及task序列化后的大小？？？

