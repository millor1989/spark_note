
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>问题记录 · GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-katex/katex.min.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="questions/dag-or-something.html" />
    
    
    <link rel="prev" href="data-analyse.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="./">
            
                <a href="./">
            
                    
                    Spark总览
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="kuai-su-ru-men.html">
            
                <a href="kuai-su-ru-men.html">
            
                    
                    快速入门
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="sparkbian-cheng-dao-yin.html">
            
                <a href="sparkbian-cheng-dao-yin.html">
            
                    
                    Spark编程导引
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="sparkbian-cheng-dao-yin/rdds.html">
            
                <a href="sparkbian-cheng-dao-yin/rdds.html">
            
                    
                    RDDs
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2" data-path="sparkbian-cheng-dao-yin/cong-java-scalaqi-dong-spark-jobs.html">
            
                <a href="sparkbian-cheng-dao-yin/cong-java-scalaqi-dong-spark-jobs.html">
            
                    
                    从Java/Scala启动Spark jobs
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="spark-sql.html">
            
                <a href="spark-sql.html">
            
                    
                    Spark SQL
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.1" data-path="spark-sql/getting-started.html">
            
                <a href="spark-sql/getting-started.html">
            
                    
                    Getting started
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2" data-path="spark-sql/shu-ju-yuan.html">
            
                <a href="spark-sql/shu-ju-yuan.html">
            
                    
                    数据源
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.3" data-path="spark-sql/xing-neng-diao-shi.html">
            
                <a href="spark-sql/xing-neng-diao-shi.html">
            
                    
                    性能调试
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.4" data-path="spark-sql/fen-bu-shi-sql-yin-qing.html">
            
                <a href="spark-sql/fen-bu-shi-sql-yin-qing.html">
            
                    
                    分布式SQL引擎
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.5" data-path="spark-sql/yu-apache-hive-de-jian-rong-xing.html">
            
                <a href="spark-sql/yu-apache-hive-de-jian-rong-xing.html">
            
                    
                    与Apache Hive的兼容性
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.6" data-path="spark-sql/shu-ju-lei-xing.html">
            
                <a href="spark-sql/shu-ju-lei-xing.html">
            
                    
                    数据类型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.7" data-path="spark-sql/built-in-functions.html">
            
                <a href="spark-sql/built-in-functions.html">
            
                    
                    内置函数
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="spark-streaming.html">
            
                <a href="spark-streaming.html">
            
                    
                    Spark Streaming
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.5.1" data-path="spark-streaming/ji-ben-gai-nian.html">
            
                <a href="spark-streaming/ji-ben-gai-nian.html">
            
                    
                    基本概念
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.2" data-path="spark-streaming/xing-neng-diao-shi.html">
            
                <a href="spark-streaming/xing-neng-diao-shi.html">
            
                    
                    性能调试
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.3" data-path="spark-streaming/rong-cuo.html">
            
                <a href="spark-streaming/rong-cuo.html">
            
                    
                    容错
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="structured-streaming.html">
            
                <a href="structured-streaming.html">
            
                    
                    Structured Streaming
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.6.1" data-path="structured-streaming/shi-yong-datasets-hedataframes-de-api.html">
            
                <a href="structured-streaming/shi-yong-datasets-hedataframes-de-api.html">
            
                    
                    使用Datasets和DataFrames的API
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.2" data-path="structured-streaming/structured-streaminghe-spark-streaming.html">
            
                <a href="structured-streaming/structured-streaminghe-spark-streaming.html">
            
                    
                    Structured Streaming和Spark Streaming
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.3" data-path="structured-streaming/003.html">
            
                <a href="structured-streaming/003.html">
            
                    
                    Structured Streaming集成Kafka
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.7" data-path="bu-shu.html">
            
                <a href="bu-shu.html">
            
                    
                    部署和运行
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.7.1" data-path="bu-shu/zu-jian-ff08-components.html">
            
                <a href="bu-shu/zu-jian-ff08-components.html">
            
                    
                    组件（Components）
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.2" data-path="bu-shu/ji-qun-guan-li-qi-lei-xing.html">
            
                <a href="bu-shu/ji-qun-guan-li-qi-lei-xing.html">
            
                    
                    集群管理器类型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.3" data-path="bu-shu/ti-jiao-ying-yong.html">
            
                <a href="bu-shu/ti-jiao-ying-yong.html">
            
                    
                    提交应用
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.4" data-path="bu-shu/jian-kong.html">
            
                <a href="bu-shu/jian-kong.html">
            
                    
                    监控
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.5" data-path="zuo-ye-diao-du.html">
            
                <a href="zuo-ye-diao-du.html">
            
                    
                    作业调度
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.6" data-path="zai-yarn-shang-yun-xing-spark.html">
            
                <a href="zai-yarn-shang-yun-xing-spark.html">
            
                    
                    在YARN上运行Spark
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.8" data-path="pei-zhi.html">
            
                <a href="pei-zhi.html">
            
                    
                    配置
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.8.1" data-path="pei-zhi/sparkshu-xing.html">
            
                <a href="pei-zhi/sparkshu-xing.html">
            
                    
                    Spark属性
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8.2" data-path="pei-zhi/dong-tai-jia-zai-spark-shu-xing.html">
            
                <a href="pei-zhi/dong-tai-jia-zai-spark-shu-xing.html">
            
                    
                    动态加载Spark属性
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8.3" data-path="pei-zhi/cha-kan-spark-shu-xing.html">
            
                <a href="pei-zhi/cha-kan-spark-shu-xing.html">
            
                    
                    查看Spark属性
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.9" data-path="diao-shi.html">
            
                <a href="diao-shi.html">
            
                    
                    调试
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.10" data-path="extff1a-xing-neng-you-hua.html">
            
                <a href="extff1a-xing-neng-you-hua.html">
            
                    
                    性能优化
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.10.1" data-path="extff1a-xing-neng-you-hua/ji-chu-you-hua.html">
            
                <a href="extff1a-xing-neng-you-hua/ji-chu-you-hua.html">
            
                    
                    基础优化
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.10.2" data-path="extff1a-xing-neng-you-hua/gao-ji-you-hua.html">
            
                <a href="extff1a-xing-neng-you-hua/gao-ji-you-hua.html">
            
                    
                    高级优化
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.11" data-path="zai-yarn-shang-yun-xing-spark.html">
            
                <a href="zai-yarn-shang-yun-xing-spark.html">
            
                    
                    在Yarn上运行Spark
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.12" data-path="zuo-ye-diao-du.html">
            
                <a href="zuo-ye-diao-du.html">
            
                    
                    作业调度
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.13" data-path="an-quan.html">
            
                <a href="an-quan.html">
            
                    
                    安全
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.14" data-path="ying-jian-pei-zhi.html">
            
                <a href="ying-jian-pei-zhi.html">
            
                    
                    硬件配置
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15" data-path="mllib.html">
            
                <a href="mllib.html">
            
                    
                    MLlib
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.15.1" data-path="mllib/shu-ju-lei-xing.html">
            
                <a href="mllib/shu-ju-lei-xing.html">
            
                    
                    数据类型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.2" data-path="mllib/ji-chu-tong-ji.html">
            
                <a href="mllib/ji-chu-tong-ji.html">
            
                    
                    基础统计
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.3" data-path="mllib/pipelines.html">
            
                <a href="mllib/pipelines.html">
            
                    
                    Pipelines
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.4" data-path="mllib/te-zheng-de-huo-qu-3001-zhuan-huan-he-shai-xuan.html">
            
                <a href="mllib/te-zheng-de-huo-qu-3001-zhuan-huan-he-shai-xuan.html">
            
                    
                    特征的提取、转换和筛选
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.15.4.1" data-path="mllib/te-zheng-de-huo-qu-3001-zhuan-huan-he-shai-xuan/feature-extractors.html">
            
                <a href="mllib/te-zheng-de-huo-qu-3001-zhuan-huan-he-shai-xuan/feature-extractors.html">
            
                    
                    Feature Extractors
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.4.2" data-path="mllib/te-zheng-de-huo-qu-3001-zhuan-huan-he-shai-xuan/feature-tranformers.html">
            
                <a href="mllib/te-zheng-de-huo-qu-3001-zhuan-huan-he-shai-xuan/feature-tranformers.html">
            
                    
                    Feature Transformers-1
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.4.3" data-path="mllib/te-zheng-de-huo-qu-3001-zhuan-huan-he-shai-xuan/feature-transformers-2.html">
            
                <a href="mllib/te-zheng-de-huo-qu-3001-zhuan-huan-he-shai-xuan/feature-transformers-2.html">
            
                    
                    Feature Transformers-2
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.4.4" data-path="mllib/te-zheng-de-huo-qu-3001-zhuan-huan-he-shai-xuan/feature-selectors.html">
            
                <a href="mllib/te-zheng-de-huo-qu-3001-zhuan-huan-he-shai-xuan/feature-selectors.html">
            
                    
                    Feature Selectors
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.4.5" data-path="mllib/te-zheng-de-huo-qu-3001-zhuan-huan-he-shai-xuan/locality-sensitive-hashing.html">
            
                <a href="mllib/te-zheng-de-huo-qu-3001-zhuan-huan-he-shai-xuan/locality-sensitive-hashing.html">
            
                    
                    Locality Sensitive Hashing
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.15.5" data-path="mllib/fen-lei-he-hui-gui.html">
            
                <a href="mllib/fen-lei-he-hui-gui.html">
            
                    
                    分类和回归
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.15.5.1" data-path="mllib/fen-lei-he-hui-gui/fen-lei.html">
            
                <a href="mllib/fen-lei-he-hui-gui/fen-lei.html">
            
                    
                    分类
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.5.2" data-path="mllib/fen-lei-he-hui-gui/hui-gui.html">
            
                <a href="mllib/fen-lei-he-hui-gui/hui-gui.html">
            
                    
                    回归
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.5.3" data-path="mllib/fen-lei-he-hui-gui/xian-xing-fang-fa.html">
            
                <a href="mllib/fen-lei-he-hui-gui/xian-xing-fang-fa.html">
            
                    
                    线性方法
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.5.4" data-path="mllib/fen-lei-he-hui-gui/jue-ce-shu.html">
            
                <a href="mllib/fen-lei-he-hui-gui/jue-ce-shu.html">
            
                    
                    决策树
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.5.5" data-path="mllib/fen-lei-he-hui-gui/shu-tuan-ti.html">
            
                <a href="mllib/fen-lei-he-hui-gui/shu-tuan-ti.html">
            
                    
                    树团体
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.15.6" data-path="mllib/ju-lei.html">
            
                <a href="mllib/ju-lei.html">
            
                    
                    聚类
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.15.7" data-path="mllib/xie-tong-guo-lv.html">
            
                <a href="mllib/xie-tong-guo-lv.html">
            
                    
                    协同过滤
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.16" data-path="data-analyse.html">
            
                <a href="data-analyse.html">
            
                    
                    数据分析
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.17" data-path="questions.html">
            
                <a href="questions.html">
            
                    
                    问题记录
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.17.1" data-path="questions/dag-or-something.html">
            
                <a href="questions/dag-or-something.html">
            
                    
                    Spark框架
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.2" data-path="questions/004.html">
            
                <a href="questions/004.html">
            
                    
                    Executors数量
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.3" data-path="questions/datasetdataframerdd.html">
            
                <a href="questions/datasetdataframerdd.html">
            
                    
                    DataFrame-Dataset-RDD
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.4" data-path="questions/spark-checkpoint.html">
            
                <a href="questions/spark-checkpoint.html">
            
                    
                    checkpoint
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.5" data-path="questions/001.html">
            
                <a href="questions/001.html">
            
                    
                    Spark Memory相关问题
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.6" data-path="questions/002.html">
            
                <a href="questions/002.html">
            
                    
                    数据倾斜和GC
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.7" data-path="questions/shu-ju-ben-di-xing.html">
            
                <a href="questions/shu-ju-ben-di-xing.html">
            
                    
                    某个task很慢
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.8" data-path="questions/sparkhan-shu.html">
            
                <a href="questions/sparkhan-shu.html">
            
                    
                    Spark函数
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.9" data-path="questions/hadoop-1he-hadoop2.html">
            
                <a href="questions/hadoop-1he-hadoop2.html">
            
                    
                    Hadoop 1和Hadoop2
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.10" data-path="questions/broadcast-join.html">
            
                <a href="questions/broadcast-join.html">
            
                    
                    Broadcast Join
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.11" data-path="questions/003.html">
            
                <a href="questions/003.html">
            
                    
                    Broadcast
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.12" data-path="questions/implicits.html">
            
                <a href="questions/implicits.html">
            
                    
                    隐式转换与隐式参数
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.17.13" data-path="questions/sss.html">
            
                <a href="questions/sss.html">
            
                    
                    Structured Streaming
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="." >问题记录</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <p>Spark&#x7684;&#x5206;&#x533A;?&#x600E;&#x4E48;&#x5206;&#x533A;&#x7684;&#xFF1F;&#x5206;&#x533A;&#x503E;&#x659C;&#x600E;&#x4E48;&#x529E;&#xFF1F;</p>
<p>Spark&#x7684;driver&#xFF0C;client&#xFF1F;</p>
<p>Spark&#x600E;&#x6837;&#x7684;&#x5206;&#x5E03;&#x5F0F;&#x8FD0;&#x884C;&#x7684;&#xFF1F;&#x8FDE;&#x63A5;Mysql&#x7B49;&#x5173;&#x7CFB;&#x578B;&#x6570;&#x636E;&#x5E93;&#x8FDB;&#x884C;&#x5206;&#x5E03;&#x5F0F;&#x8FD0;&#x7B97;&#x65F6;&#xFF0C;&#x4F1A;&#x4E0D;&#x4F1A;&#x9020;&#x6210;&#x6570;&#x636E;&#x5E93;&#x7684;&#x8FDE;&#x63A5;&#x8FC7;&#x591A;&#x3002;</p>
<p>dataset&#x7684;&#x5206;&#x533A;&#x6570;&#xFF0C;&#x521D;&#x59CB;&#x5206;&#x533A;&#x6570;</p>
<p>Spark Streaming&#x7684;&#x6279;&#x6B21;&#x5927;&#x5C0F;&#x600E;&#x4E48;&#x786E;&#x5B9A;&#xFF1F;</p>
<p><code>spark.sparkContext.defaultParallelism</code>&#x9ED8;&#x8BA4;&#x7684;&#x5E76;&#x884C;&#x5EA6;</p>
<p>Kafka&#x4F5C;&#x7528;&#xFF0C;&#x524A;&#x5CF0;&#x5F02;&#x6B65;&#x89E3;&#x8026;</p>
<h4 id="spark&#x7A0B;&#x5E8F;&#x914D;&#x7F6E;&#x4E2D;&#x7684;&#x6DF7;&#x6D17;&#x5206;&#x533A;&#x6570;">spark&#x7A0B;&#x5E8F;&#x914D;&#x7F6E;&#x4E2D;&#x7684;&#x6DF7;&#x6D17;&#x5206;&#x533A;&#x6570;</h4>
<p>&#x6DF7;&#x6D17;&#x5206;&#x533A;&#x591A;&#x4F1A;&#x5BFC;&#x81F4;&#x5185;&#x5B58;&#x4F7F;&#x7528;&#x8FC7;&#x591A;&#x5417;&#xFF1F;&#x4F1A;&#x5BFC;&#x81F4;GC&#x95EE;&#x9898;&#x7684;&#x673A;&#x7387;&#x53D8;&#x5927;&#x5417;&#xFF1F;200&#x4E07;&#x6570;&#x636E;&#x7684;join&#xFF0C;&#x6DF7;&#x6D17;&#x5206;&#x533A;50&#x4F1A;GC&#x95EE;&#x9898;(Exception in thread &quot;dispatcher-event-loop-8&quot; java.lang.OutOfMemoryError: GC overhead limit exceeded)&#xFF0C;&#x6DF7;&#x6D17;&#x5206;&#x533A;&#x4E3A;10&#x5219;&#x6CA1;&#x6709;GC&#x95EE;&#x9898;&#x3002;&#x5177;&#x4F53;&#x539F;&#x56E0;&#x5F85;&#x9A8C;&#x8BC1;&#x3002;</p>
<p>spark&#x914D;&#x7F6E;&#x5206;&#x522B;&#x5982;&#x4E0B;&#xFF1A;</p>
<p>&#x6DF7;&#x6D17;&#x5206;&#x533A;50&#x65F6;&#xFF1A;</p>
<pre><code>config(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)
.config(&quot;hive.metastore.uris&quot;, &quot;thrift://node001.cdh.jdd.com:9083&quot;)
.config(&quot;spark.dynamicAllocation.enabled&quot;, &quot;false&quot;)
.config(&quot;spark.debug.maxToStringFields&quot;, &quot;10000&quot;)
.config(&quot;spark.sql.adaptive.enabled&quot;, &quot;true&quot;)
.config(&quot;spark.sql.adaptive.minNumPostShufflePartitions&quot;, &quot;50&quot;)
.config(&quot;spark.sql.adaptive.maxNumPostShufflePartitions&quot;, &quot;500&quot;)
.config(&quot;spark.sql.adaptive.shuffle.targetPostShuffleInputSize&quot;, &quot;67108864&quot;)
.config(&quot;spark.sql.autoBroadcastJoinThreshold&quot;, &quot;200485760&quot;)
.config(&quot;spark.sql.broadcastTimeout&quot;, 1200)
.config(&quot;spark.locality.wait&quot;, &quot;6&quot;)
.config(&quot;spark.shuffle.file.buffer&quot;, &quot;64&quot;)
.config(&quot;spark.reducer.maxSizeInFlight&quot;, &quot;96&quot;)
</code></pre><p>&#x6DF7;&#x6D17;&#x5206;&#x533A;10&#x65F6;</p>
<pre><code>sparkConf.set(&quot;hive.exec.dynamic.partition&quot;, &quot;true&quot;); //&#x652F;&#x6301;&#x52A8;&#x6001;&#x5206;&#x533A;
        sparkConf.set(&quot;hive.exec.max.created.files&quot;, &quot;100000&quot;); //&#x8BBE;&#x7F6E;&#x80FD;&#x591F;&#x521B;&#x5EFA;&#x6700;&#x5927;&#x7684;&#x6587;&#x4EF6;&#x6570;
        sparkConf.set(&quot;hive.exec.max.dynamic.partitions&quot;, &quot;100000&quot;); //&#x8BBE;&#x7F6E;&#x4E00;&#x4E2A;dml&#x8BED;&#x53E5;&#x5141;&#x8BB8;&#x521B;&#x5EFA;&#x7684;&#x6240;&#x6709;&#x5206;&#x533A;&#x7684;&#x6700;&#x5927;&#x6570;&#x91CF;
        sparkConf.set(&quot;hive.exec.dynamic.partition.mode&quot;, &quot;nonstrict&quot;); //
        sparkConf.set(&quot;hive.exec.max.dynamic.partitions.pernode&quot;, &quot;100000&quot;); //&#x6BCF;&#x4E00;&#x4E2A;mapreduce job&#x5141;&#x8BB8;&#x521B;&#x5EFA;&#x7684;&#x5206;&#x533A;&#x7684;&#x6700;&#x5927;&#x6570;&#x91CF;
        sparkConf.set(&quot;dfs.permissions&quot;, &quot;false&quot;);
        sparkConf.set(&quot;hive.metastore.uris&quot;, &quot;thrift://node001.cdh.jdd.com:9083&quot;);
        sparkConf.set(&quot;spark.network.timeout&quot;, &quot;360&quot;);
        sparkConf.set(&quot;spark.sql.shuffle.partitions&quot;, &quot;10&quot;);
        sparkConf.set(&quot;spark.shuffle.service.enabled&quot;, &quot;true&quot;);
        sparkConf.set(&quot;spark.dynamicAllocation.enabled&quot;, &quot;false&quot;); //&#x52A8;&#x6001;&#x7533;&#x8BF7;executor&#xFF0C;&#x9ED8;&#x8BA4;true
        sparkConf.set(&quot;spark.yarn.executor.memoryOverhead&quot;, &quot;3072&quot;); // M
        sparkConf.set(&quot;spark.sql.autoBroadcastJoinThreshold&quot;, &quot;5000&quot;);
        sparkConf.set(&quot;spark.debug.maxToStringFields&quot;, &quot;5000&quot;);
        sparkConf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;);
        sparkConf.set(&quot;mapreduce.job.inputformat.class&quot;, &quot;org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat&quot;);
        sparkConf.set(&quot;mapreduce.input.fileinputformat.split.maxsize&quot;, &quot;268435456&quot;);
        sparkConf.set(&quot;spark.kryoserializer.buffer.max&quot;, &quot;1024m&quot;);
        sparkConf.set(&quot;spark.driver.maxResultSize&quot;, &quot;4096&quot;);
</code></pre><p><code>spark.default.parallelism</code>&#xFF0C;&#x662F;&#x5982;<code>join</code>&#xFF0C;<code>reduceByKey</code>&#xFF0C;<code>parallelize</code>&#x8FD9;&#x4E9B;&#x8F6C;&#x6362;&#x64CD;&#x4F5C;&#x8FD4;&#x56DE;&#x7684;RDD&#x7684;&#x9ED8;&#x8BA4;&#x5206;&#x533A;&#x6570;&#x3002;&#x5E76;&#x4E14;<code>spark.default.parallelism</code>&#x53EA;&#x5BF9;RDD&#x8D77;&#x4F5C;&#x7528;&#xFF0C;&#x5BF9;dataframe&#x4E0D;&#x8D77;&#x4F5C;&#x7528;&#x3002;</p>
<p><code>spark.sql.shuffle.partitions</code>&#x914D;&#x7F6E;joins&#x6216;aggregations&#x6DF7;&#x6D17;&#x65F6;&#x4F7F;&#x7528;&#x7684;&#x5206;&#x533A;&#x6570;&#x3002;</p>
<p>&#x5206;&#x533A;&#x6570;&#x591A;&#x5C11;&#x7684;&#x5F71;&#x54CD;&#xFF1A;&#x5206;&#x533A;&#x592A;&#x5C11;&#x4E0D;&#x80FD;&#x5145;&#x5206;&#x5229;&#x7528;&#x96C6;&#x7FA4;&#x7684;&#x8D44;&#x6E90;&#xFF0C;&#x5BF9;&#x7A0B;&#x5E8F;&#x7684;&#x6267;&#x884C;&#x6548;&#x7387;&#x5F71;&#x54CD;&#x8F83;&#x5927;&#xFF1B;&#x5206;&#x533A;&#x592A;&#x591A;&#xFF0C;&#x4F1A;&#x5728;&#x7BA1;&#x7406;&#x8BB8;&#x591A;&#x8BB8;&#x591A;&#x7684;&#x5C0F;tasks&#x4E0A;&#x4EA7;&#x751F;&#x8FC7;&#x591A;&#x7684;&#x5F00;&#x9500;&#xFF0C;1000&#x4E2A;&#x4EE5;&#x4E0B;&#x7684;&#x5206;&#x533A;&#x6570;&#xFF0C;&#x4E0D;&#x4F1A;&#x5BF9;&#x7A0B;&#x5E8F;&#x6548;&#x7387;&#x4EA7;&#x751F;&#x592A;&#x5927;&#x5F71;&#x54CD;&#xFF0C;&#x4F46;&#x662F;&#x5982;&#x679C;&#x6709;&#x6570;&#x4E07;&#x4E2A;&#x5219;&#x4F1A;&#x5BFC;&#x81F4;&#x7A0B;&#x5E8F;&#x53D8;&#x5F97;&#x975E;&#x5E38;&#x6162;&#x3002;&#x9ED8;&#x8BA4;&#x5206;&#x533A;&#x6570;&#x53EF;&#x4EE5;&#x8BBE;&#x7F6E;&#x4E3A;&#x6838;&#x5FC3;&#x6570;&#x7684;2~4&#x500D;&#x3002;</p>
<h4 id="&#x5E7F;&#x64AD;&#x53D8;&#x91CF;&#x7684;&#x4F7F;&#x7528;">&#x5E7F;&#x64AD;&#x53D8;&#x91CF;&#x7684;&#x4F7F;&#x7528;</h4>
<h4 id="inner-join&#xFF0C;&#x591A;&#x5B57;&#x6BB5;&#x5173;&#x8054;&#xFF0C;&#x4E24;&#x4E2A;30&#x4E07;&#x5DE6;&#x53F3;&#x7684;&#x6570;&#x636E;&#x96C6;&#xFF0C;&#x7ED3;&#x679C;&#x7ADF;&#x7136;90&#x591A;&#x4EBF;&#x6761;&#x6570;&#x636E;&#xFF0C;&#x7528;distinct&#x89E3;&#x51B3;">inner join&#xFF0C;&#x591A;&#x5B57;&#x6BB5;&#x5173;&#x8054;&#xFF0C;&#x4E24;&#x4E2A;30&#x4E07;&#x5DE6;&#x53F3;&#x7684;&#x6570;&#x636E;&#x96C6;&#xFF0C;&#x7ED3;&#x679C;&#x7ADF;&#x7136;90&#x591A;&#x4EBF;&#x6761;&#x6570;&#x636E;&#xFF0C;&#x7528;distinct&#x89E3;&#x51B3;</h4>
<h4 id="spark&#x7A0B;&#x5E8F;&#x8DD1;&#x7684;&#x6162;&#xFF0C;&#x8FDE;&#x63A5;&#x6267;&#x884C;&#x6162;&#xFF0C;&#x53EF;&#x80FD;&#x53EA;&#x662F;&#x56E0;&#x4E3A;&#x5C11;&#x4E86;&#x4E00;&#x4E2A;distinct&#x3002;">Spark&#x7A0B;&#x5E8F;&#x8DD1;&#x7684;&#x6162;&#xFF0C;&#x8FDE;&#x63A5;&#x6267;&#x884C;&#x6162;&#xFF0C;&#x53EF;&#x80FD;&#x53EA;&#x662F;&#x56E0;&#x4E3A;&#x5C11;&#x4E86;&#x4E00;&#x4E2A;distinct&#x3002;</h4>
<p>Spark&#x4F18;&#x5316;&#xFF1A;&#x57FA;&#x4E8E;&#x5206;&#x533A;&#xFF08;&#x76F8;&#x6BD4;&#x57FA;&#x4E8E;&#x6BCF;&#x6761;&#x8BB0;&#x5F55;&#xFF0C;&#x51CF;&#x5C11;&#x914D;&#x7F6E;&#x6B21;&#x6570;&#xFF1B;&#x76F8;&#x5BF9;&#x4E8E;collect&#x5230;driver&#x4E2D;&#xFF0C;&#x907F;&#x514D;&#x4E86;&#x6570;&#x636E;&#x91CF;&#x8FC7;&#x5927;&#x5BFC;&#x81F4;&#x7684;OOM&#xFF09;</p>
<p>Spark Sql &#x81EA;&#x9002;&#x5E94;&#x6267;&#x884C;&#xFF1A;</p>
<ul>
<li>shuffle partition&#x4E2A;&#x6570;</li>
<li>&#x6570;&#x636E;&#x503E;&#x659C;</li>
<li>Runtime&#x6267;&#x884C;&#x8BA1;&#x5212;&#x4F18;&#x5316;</li>
</ul>
<p><a href="https://help.aliyun.com/document_detail/93157.html" target="_blank">https://help.aliyun.com/document_detail/93157.html</a></p>
<p><a href="http://www.jasongj.com/spark/adaptive_execution/" target="_blank">http://www.jasongj.com/spark/adaptive_execution/</a></p>
<p>Spark SQL&#x67B6;&#x6784;&#xFF1A;</p>
<p><img src="assets/sparksql&#x67B6;&#x6784;.png" alt=""></p>
<p>&#x4ECE;&#x4E0A;&#x56FE;&#x53EF;&#x89C1;&#xFF0C;&#x65E0;&#x8BBA;&#x662F;&#x76F4;&#x63A5;&#x4F7F;&#x7528; SQL &#x8BED;&#x53E5;&#x8FD8;&#x662F;&#x4F7F;&#x7528; DataFrame&#xFF0C;&#x90FD;&#x4F1A;&#x7ECF;&#x8FC7;&#x5982;&#x4E0B;&#x6B65;&#x9AA4;&#x8F6C;&#x6362;&#x6210; DAG &#x5BF9; RDD &#x7684;&#x64CD;&#x4F5C;</p>
<ul>
<li>Parser &#x89E3;&#x6790; SQL&#xFF0C;&#x751F;&#x6210; Unresolved Logical Plan</li>
<li>&#x7531; Analyzer &#x7ED3;&#x5408; Catalog &#x4FE1;&#x606F;&#x751F;&#x6210; Resolved Logical Plan</li>
<li>Optimizer&#x6839;&#x636E;&#x9884;&#x5148;&#x5B9A;&#x4E49;&#x597D;&#x7684;&#x89C4;&#x5219;&#x5BF9; Resolved Logical Plan &#x8FDB;&#x884C;&#x4F18;&#x5316;&#x5E76;&#x751F;&#x6210; Optimized Logical Plan</li>
<li>Query Planner &#x5C06; Optimized Logical Plan &#x8F6C;&#x6362;&#x6210;&#x591A;&#x4E2A; Physical Plan</li>
<li>CBO &#x6839;&#x636E; Cost Model &#x7B97;&#x51FA;&#x6BCF;&#x4E2A; Physical Plan &#x7684;&#x4EE3;&#x4EF7;&#x5E76;&#x9009;&#x53D6;&#x4EE3;&#x4EF7;&#x6700;&#x5C0F;&#x7684; Physical Plan &#x4F5C;&#x4E3A;&#x6700;&#x7EC8;&#x7684; Physical Plan</li>
<li>Spark &#x4EE5; DAG &#x7684;&#x65B9;&#x6CD5;&#x6267;&#x884C;&#x4E0A;&#x8FF0; Physical Plan</li>
<li>&#x5728;&#x6267;&#x884C; DAG &#x7684;&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;Adaptive Execution &#x6839;&#x636E;&#x8FD0;&#x884C;&#x65F6;&#x4FE1;&#x606F;&#x52A8;&#x6001;&#x8C03;&#x6574;&#x6267;&#x884C;&#x8BA1;&#x5212;&#x4ECE;&#x800C;&#x63D0;&#x9AD8;&#x6267;&#x884C;&#x6548;&#x7387;</li>
</ul>
<p>&#x5728;Spark SQL&#x4E2D;&#xFF0C;&#x7531;<code>AstBuilder</code>&#x6765;&#x6784;&#x5EFA;&#x4E00;&#x4E2A;&#x903B;&#x8F91;&#x7B97;&#x5B50;&#x548C;&#x8868;&#x8FBE;&#x5F0F;&#x7EC4;&#x6210;&#x7684;&#x62BD;&#x8C61;&#x8BED;&#x6CD5;&#x6811;&#x3002;</p>
<blockquote>
<p>AstBuilder&#x662F;&#x4E00;&#x4E2A;&#x57FA;&#x4E8E;ANTLR&#x7684;SQL&#x89E3;&#x6790;&#x5668;&#xFF0C;&#x4F7F;&#x7528;&#x7684;&#x662F;<code>SqlBase.g4</code>&#x6587;&#x4EF6;&#x4E2D;&#x63CF;&#x8FF0;&#x7684;SQL&#x8BED;&#x6CD5;&#xFF0C;&#x628A;&#x4E00;&#x4E2A;ANTLR4 ParseTree&#x8F6C;&#x6362;&#x4E3A;&#x4E00;&#x4E2A;catalyst <code>Expression</code>&#xFF0C;<code>LogicalPlan</code>&#x6216;&#x8005;<code>TableIdentifier</code>&#x3002;</p>
</blockquote>
<p>&#x901A;&#x8FC7;<code>SparkSession.sql(...)</code>&#x65B9;&#x6CD5;&#x7684;&#x63CF;&#x8FF0;&#xFF0C;&#x901A;&#x8FC7;<code>spark.sql.dialect</code>&#x53EF;&#x4EE5;&#x8BBE;&#x7F6E;&#x7528;&#x4E8E;sql&#x89E3;&#x6790;&#x7684;&#x65B9;&#x8A00;&#xFF1A;</p>
<pre><code class="lang-scala">  <span class="hljs-comment">/**
   * Executes a SQL query using Spark, returning the result as a `DataFrame`.
   * The dialect that is used for SQL parsing can be configured with &apos;spark.sql.dialect&apos;.
   */</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sql</span></span>(sqlText: <span class="hljs-type">String</span>): <span class="hljs-type">DataFrame</span> = {
    <span class="hljs-type">Dataset</span>.ofRows(self, sessionState.sqlParser.parsePlan(sqlText))
  }
</code></pre>
<p>&#x5C1D;&#x8BD5;&#x4E00;&#x4E0B;&#x4F7F;&#x7528;<code>sqlParser</code>&#x6765;&#x83B7;&#x53D6;Logical Plan&#xFF1A;</p>
<pre><code class="lang-scala">scala&gt; <span class="hljs-keyword">import</span> spark.sessionState.sqlParser
<span class="hljs-keyword">import</span> spark.sessionState.sqlParser

scala&gt; sqlParser.parsePlan(<span class="hljs-string">&quot;select * from ids_ttable&quot;</span>)
res2: org.apache.spark.sql.catalyst.plans.logical.<span class="hljs-type">LogicalPlan</span> =
<span class="hljs-symbol">&apos;Project</span> [*]
+- <span class="hljs-symbol">&apos;UnresolvedRelation</span> `ids_ttable`
</code></pre>
<p>Spark SQL&#x7684;&#x4F7F;&#x7528;<code>QueryExecution</code>&#x6765;&#x5904;&#x7406;Logcial Plan&#xFF0C;<code>QueryExecution</code>&#x662F;&#x4E00;&#x4E2A;&#x7ED3;&#x6784;&#x5316;&#x7684;&#x67E5;&#x8BE2;&#x6267;&#x884C;&#x5DE5;&#x4F5C;&#x6D41;&#xFF08;a structured query execution pipeline&#xFF09;&#xFF1A;</p>
<blockquote>
<p>QueryExecution&#xFF1A;&#x4F7F;&#x7528;Spark&#x6267;&#x884C;&#x5173;&#x7CFB;&#x67E5;&#x8BE2;&#xFF08;relational queries&#xFF09;&#x7684;&#x4E3B;&#x8981;&#x5DE5;&#x4F5C;&#x6D41;&#x3002;&#x5F00;&#x53D1;&#x8005;&#x53EF;&#x4EE5;&#x8F7B;&#x677E;&#x7684;&#x8BBF;&#x95EE;query execution&#x7684;&#x4E2D;&#x95F4;&#x9636;&#x6BB5;&#x3002;&#x53EF;&#x4EE5;&#x7528;&#x6765;debug&#x3002;</p>
</blockquote>
<p>&#x67E5;&#x770B;&#x4E00;&#x4E2A;<code>Dataset</code>&#x7684;<code>QueryExecution</code>&#xFF1A;</p>
<pre><code class="lang-scala">scala&gt; <span class="hljs-keyword">val</span> ds = spark.sql(<span class="hljs-string">&quot;select * from ids_ttable limit 10&quot;</span>)
ds: org.apache.spark.sql.<span class="hljs-type">DataFrame</span> = [id: int]

scala&gt; <span class="hljs-keyword">val</span> qe = ds.queryExecution
qe: org.apache.spark.sql.execution.<span class="hljs-type">QueryExecution</span> =
== <span class="hljs-type">Parsed</span> <span class="hljs-type">Logical</span> <span class="hljs-type">Plan</span> ==
<span class="hljs-symbol">&apos;GlobalLimit</span> <span class="hljs-number">10</span>
+- <span class="hljs-symbol">&apos;LocalLimit</span> <span class="hljs-number">10</span>
   +- <span class="hljs-symbol">&apos;Project</span> [*]
      +- <span class="hljs-symbol">&apos;UnresolvedRelation</span> `ids_ttable`

== <span class="hljs-type">Analyzed</span> <span class="hljs-type">Logical</span> <span class="hljs-type">Plan</span> ==
id: int
<span class="hljs-type">GlobalLimit</span> <span class="hljs-number">10</span>
+- <span class="hljs-type">LocalLimit</span> <span class="hljs-number">10</span>
   +- <span class="hljs-type">Project</span> [id#<span class="hljs-number">3</span>]
      +- <span class="hljs-type">SubqueryAlias</span> ids_ttable
         +- <span class="hljs-type">Project</span> [value#<span class="hljs-number">1</span> <span class="hljs-type">AS</span> id#<span class="hljs-number">3</span>]
            +- <span class="hljs-type">LocalRelation</span> [value#<span class="hljs-number">1</span>]

== <span class="hljs-type">Optimized</span> <span class="hljs-type">Logical</span> <span class="hljs-type">Plan</span> ==
<span class="hljs-type">GlobalLimit</span> <span class="hljs-number">10</span>
+- <span class="hljs-type">LocalLimit</span> <span class="hljs-number">10</span>
   +- <span class="hljs-type">LocalRelation</span> [id#<span class="hljs-number">3</span>]

== <span class="hljs-type">Physical</span> <span class="hljs-type">Plan</span> ==
<span class="hljs-type">CollectLimit</span> <span class="hljs-number">10</span>
+- <span class="hljs-type">LocalTableScan</span> [id#<span class="hljs-number">3</span>]

<span class="hljs-comment">/**&#x4E5F;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;Dataset&#x7684;explain&#x51FD;&#x6570;*/</span>
scala&gt; ds.explain(<span class="hljs-literal">true</span>)
== <span class="hljs-type">Parsed</span> <span class="hljs-type">Logical</span> <span class="hljs-type">Plan</span> ==
<span class="hljs-symbol">&apos;GlobalLimit</span> <span class="hljs-number">10</span>
+- <span class="hljs-symbol">&apos;LocalLimit</span> <span class="hljs-number">10</span>
   +- <span class="hljs-symbol">&apos;Project</span> [*]
      +- <span class="hljs-symbol">&apos;UnresolvedRelation</span> `ids_ttable`

== <span class="hljs-type">Analyzed</span> <span class="hljs-type">Logical</span> <span class="hljs-type">Plan</span> ==
id: int
<span class="hljs-type">GlobalLimit</span> <span class="hljs-number">10</span>
+- <span class="hljs-type">LocalLimit</span> <span class="hljs-number">10</span>
   +- <span class="hljs-type">Project</span> [id#<span class="hljs-number">3</span>]
      +- <span class="hljs-type">SubqueryAlias</span> ids_ttable
         +- <span class="hljs-type">Project</span> [value#<span class="hljs-number">1</span> <span class="hljs-type">AS</span> id#<span class="hljs-number">3</span>]
            +- <span class="hljs-type">LocalRelation</span> [value#<span class="hljs-number">1</span>]

== <span class="hljs-type">Optimized</span> <span class="hljs-type">Logical</span> <span class="hljs-type">Plan</span> ==
<span class="hljs-type">GlobalLimit</span> <span class="hljs-number">10</span>
+- <span class="hljs-type">LocalLimit</span> <span class="hljs-number">10</span>
   +- <span class="hljs-type">LocalRelation</span> [id#<span class="hljs-number">3</span>]

== <span class="hljs-type">Physical</span> <span class="hljs-type">Plan</span> ==
<span class="hljs-type">CollectLimit</span> <span class="hljs-number">10</span>
+- <span class="hljs-type">LocalTableScan</span> [id#<span class="hljs-number">3</span>]
</code></pre>
<h3 id="spark&#x7684;accept&#x72B6;&#x6001;&#xFF0C;&#x90FD;&#x5728;&#x505A;&#x4EC0;&#x4E48;&#xFF1F;&#x53EA;&#x662F;&#x7B49;&#x5F85;&#x8D44;&#x6E90;&#xFF1F;&#x4E3A;&#x4EC0;&#x4E48;&#x4F1A;&#x7B49;&#x5F85;&#x5F88;&#x4E45;&#x2014;&#x2014;&#x51E0;&#x4E2A;&#x5C0F;&#x65F6;&#xFF1F;&#x5927;&#x6982;&#x662F;&#x7B49;&#x8D44;&#x6E90;&#x5427;&#x3002;">Spark&#x7684;Accept&#x72B6;&#x6001;&#xFF0C;&#x90FD;&#x5728;&#x505A;&#x4EC0;&#x4E48;&#xFF1F;&#x53EA;&#x662F;&#x7B49;&#x5F85;&#x8D44;&#x6E90;&#xFF1F;&#x4E3A;&#x4EC0;&#x4E48;&#x4F1A;&#x7B49;&#x5F85;&#x5F88;&#x4E45;&#x2014;&#x2014;&#x51E0;&#x4E2A;&#x5C0F;&#x65F6;&#xFF1F;&#x5927;&#x6982;&#x662F;&#x7B49;&#x8D44;&#x6E90;&#x5427;&#x3002;</h3>
<p>Accept&#x72B6;&#x6001;&#x65F6;&#x662F;&#x5728;&#x6267;&#x884C;&#x521D;&#x59CB;&#x5316;SparkSession&#x4E4B;&#x524D;&#x7684;&#x4EE3;&#x7801;&#xFF0C;&#x4EE5;&#x53CA;&#x5206;&#x914D;&#x8D44;&#x6E90;&#x8FDB;&#x884C;SparkSession&#x7684;&#x521D;&#x59CB;&#x5316;&#x3002;</p>
<p>&#x4ECE;&#x65E5;&#x5FD7;&#x63A8;&#x6D4B;&#xFF1A;&#x9996;&#x5148;&#x542F;&#x52A8;ApplicationMaster&#xFF1A;Preparing Local resources&#xFF1B;&#x542F;&#x52A8;<code>ApplicationAttemptId</code>&#xFF1B;&#x542F;&#x52A8;user application&#xFF1B;&#x521D;&#x59CB;&#x5316;spark context&#xFF1B;spark context&#x63D0;&#x4EA4;&#x5E94;&#x7528;&#xFF1B;&#x542F;&#x52A8;sparkDriver&#xFF1B;&#x83B7;&#x53D6;&#x8D44;&#x6E90;&#x542F;&#x52A8;executor&#xFF0C;&#x4E4B;&#x524D;&#x662F;&#x4E0D;&#x662F;&#x5148;&#x542F;&#x52A8;&#x5BB9;&#x5668;&#x5462;&#xFF1F;&#x518D;&#x4E4B;&#x524D;YARN&#x8C03;&#x5EA6;&#x5462;&#x2026;&#x2026;</p>
<h3 id="event-timeline&#x51FA;&#x73B0;&#x4E86;&#x5DE8;&#x957F;&#x7684;&#x7A7A;&#x767D;&#xFF0C;&#x600E;&#x4E48;&#x56DE;&#x4E8B;&#x5462;&#xFF1F;&#x751F;&#x6210;&#x6267;&#x884C;&#x8BA1;&#x5212;&#xFF1F;&#x751F;&#x6210;sql&#x5417;">Event Timeline&#x51FA;&#x73B0;&#x4E86;&#x5DE8;&#x957F;&#x7684;&#x7A7A;&#x767D;&#xFF0C;&#x600E;&#x4E48;&#x56DE;&#x4E8B;&#x5462;&#xFF1F;&#x751F;&#x6210;&#x6267;&#x884C;&#x8BA1;&#x5212;&#xFF1F;&#x751F;&#x6210;sql&#x5417;?</h3>
<p><img src="assets/eventtimelinewhitespace.png" alt=""></p>
<h3 id="&#x89C6;&#x56FE;">&#x89C6;&#x56FE;</h3>
<ul>
<li><h4 id="local-temporary-view">Local Temporary View</h4>
<p>Local temporary view is session-scoped. Its lifetime is the lifetime of the session that create it,i.e. it will be automatically dropped when the session terminates. It&apos;s not tied to any databases, i.e. we can&apos;t use `db1.view1` to reference a local temporary view.</p>
</li>
<li><h4 id="global-temporary-view">Global temporary view</h4>
<p>Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application, i.e. it will be automatically dropped when the application terminates. It&apos;s tied to a system preserved database `global_temp`, and we must use the qualified name to refer a global temp view, e.g. `SELECT * FROM global_temp.view1`.</p>
</li>
</ul>
<pre><code class="lang-scala"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dropTempView</span></span>(viewName: <span class="hljs-type">String</span>): <span class="hljs-type">Boolean</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dropGlobalTempView</span></span>(viewName: <span class="hljs-type">String</span>): <span class="hljs-type">Boolean</span>
</code></pre>
<p>This method drops the view with the given view name in the catalog. If the view has been cached before, then it will also be uncached.</p>
<p>Spark SQL&#x8FD0;&#x884C;&#x8FC7;&#x7A0B;&#x4E2D;&#x662F;&#x5426;&#x4E5F;&#x4F1A;&#x6839;&#x636E;&#x60C5;&#x51B5;&#x81EA;&#x52A8;Cache&#x4E00;&#x4E9B;RDD&#xFF1F;&#xFF1F;&#x53D1;&#x73B0;<code>dropTempView</code>&#x548C;<code>unpersist</code>&#x8FD8;&#x662F;&#x4F1A;&#x6E05;&#x9664;&#x4E0D;&#x6389;&#x7F13;&#x5B58;&#xFF08;Spark Web UI&#x7684;Storage&#x4E2D;&#x7684;RDD&#x7F13;&#x5B58;&#x4E00;&#x81F4;&#x90FD;&#x5B58;&#x5728;&#xFF09;&#xFF0C;&#x4F46;&#x662F;&#x6539;&#x7528;<code>spark.catalog().clearCache()</code>&#xFF08;&#x4ECE;&#x5185;&#x5B58;&#x4E2D;&#x5220;&#x9664;&#x6240;&#x6709;&#x7F13;&#x5B58;&#x7684;&#x8868;&#xFF09;&#x53EF;&#x4EE5;&#x5C06;&#x5B83;&#x4EEC;&#x6E05;&#x9664;&#x3002;</p>
<h4 id="spark-web-ui&#x7684;executors&#x4E2D;&#xFF0C;storage-memory&#x4E00;&#x76F4;&#x5728;&#x589E;&#x52A0;&#xFF08;increasing-overtime&#xFF09;">Spark Web UI&#x7684;&#x201C;Executors&#x201D;&#x4E2D;&#xFF0C;Storage Memory&#x4E00;&#x76F4;&#x5728;&#x589E;&#x52A0;&#xFF08;increasing overtime&#xFF09;</h4>
<p><img src="assets/1616741331350.png" alt="1616741331350"></p>
<p>&#x7A0B;&#x5E8F;&#x8FD0;&#x884C;2&#x4E2A;&#x5C0F;&#x65F6;&#x5DE6;&#x53F3;&#xFF0C;Executors&#x7684;Storage Memeory&#x4E00;&#x76F4;&#x5728;&#x589E;&#x52A0;&#xFF0C;&#x6700;&#x521D;&#x6000;&#x7591;&#x662F;&#x5927;&#x91CF;&#x7684;DataFrame cache&#x5BFC;&#x81F4;&#x7684;&#xFF0C;&#x4F46;&#x662F;&#x8FDB;&#x884C;&#x4E86;<code>spark.catalog().clearCache()</code>&#x64CD;&#x4F5C;&#x540E;&#xFF0C;&#x6E05;&#x9664;RDD&#x7684;&#x7F13;&#x5B58;&#xFF0C;&#x800C;Storage Memeory&#x4F9D;&#x65E7;&#x5728;&#x589E;&#x957F;&#x3002;</p>
<p>&#x4E0D;&#x77E5;&#x9053;&#x662F;&#x4E0D;&#x662F;Spark Web UI&#x7684;bug&#xFF0C;&#x8FD8;&#x662F;&#x8BF4;&#x662F;&#x5185;&#x5B58;&#x6CC4;&#x9732;&#x5BFC;&#x81F4;&#x7684;&#xFF1F;&#xFF1F;</p>
<p>&#x4E4B;&#x524D;&#x6CA1;&#x6709;&#x6CE8;&#x610F;&#x8FC7;&#xFF0C;&#x4F3C;&#x4E4E;&#x6240;&#x6709;&#x7684;Spark&#x5E94;&#x7528;&#x7684;Storage Memeory&#x90FD;&#x5B58;&#x5728;&#x9012;&#x589E;&#x7684;&#x60C5;&#x51B5;&#x3002;&#x4F46;&#x662F;Storage Memory&#x8FC7;&#x5927;&#xFF0C;&#x5E94;&#x8BE5;&#x8FD8;&#x662F;&#x6709;&#x9700;&#x8981;&#x4F18;&#x5316;&#x7684;&#x5730;&#x65B9;&#x3002;</p>
<h4 id="spark-executor-gc-time">Spark Executor GC Time</h4>
<h4 id="hiveimpala&#x7684;&#x5B9E;&#x65F6;&#x6027;&#x3002;alter-table&#x7684;&#x539F;&#x56E0;&#x5417;&#xFF1F;&#x611F;&#x89C9;&#x4FEE;&#x6539;&#x6570;&#x636E;&#x540E;&#xFF0C;&#x67E5;&#x8BE2;&#x8868;&#x7684;&#x4FEE;&#x6B63;&#x7ED3;&#x679C;&#x4F1A;&#x6709;&#x5EF6;&#x65F6;&#x3002;">Hive/Impala&#x7684;&#x5B9E;&#x65F6;&#x6027;&#x3002;ALTER TABLE&#x7684;&#x539F;&#x56E0;&#x5417;&#xFF1F;&#x611F;&#x89C9;&#x4FEE;&#x6539;&#x6570;&#x636E;&#x540E;&#xFF0C;&#x67E5;&#x8BE2;&#x8868;&#x7684;&#x4FEE;&#x6B63;&#x7ED3;&#x679C;&#x4F1A;&#x6709;&#x5EF6;&#x65F6;&#x3002;</h4>
<h3 id="resolved-attributes-xxxxxxxx-missing-from">resolved attribute(s) xxxxx#xxx missing from...</h3>
<pre><code>20/02/27 12:11:04 ERROR yarn.ApplicationMaster: User class threw exception: org.apache.spark.sql.AnalysisException: resolved attribute(s) recharge_level#738 missing from app_id#300L,is_parent_channel#336,recharge_level#113,user_id#298L,parent_id#337L,channel_id#257L in operator !Project [user_id#298L, app_id#300L, channel_id#257L, is_parent_channel#336, parent_id#337L, recharge_level#738];;
Project [app_id#84L, channel_id#80L, is_parent_channel#65, recharge_level#113, dau#412L, channel_user_count#414L, level_in#349L, level_out#360L]
+- Join LeftOuter, ((((app_id#84L = app_id#825L) &amp;&amp; (channel_id#80L = channel_id#782L)) &amp;&amp; (is_parent_channel#65 = is_parent_channel#861)) &amp;&amp; (recharge_level#113 = recharge_level#738))
&#x2026;&#x2026;
&#x2026;&#x2026;
&#x2026;&#x2026;
                                       :                 +- CatalogRelation `dw_wf_fish`.`channel_info`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [id#67L, parent_id#68L, code#69, name#70, description#71, enable#72, create_time#73, update_time#74, delete_flag#75]
                                       +- Project [user_id#34L, recharge_level#60 AS p_level#128]
                                          +- Project [user_id#34L, CASE WHEN ((vip_level#32 = 0) &amp;&amp; (cast(total_recharge_money#33 as decimal(18,2)) = cast(cast(0 as decimal(10,0)) as decimal(18,2)))) THEN -1 WHEN ((vip_level#32 = 0) &amp;&amp; NOT (cast(total_recharge_money#33 as decimal(18,2)) = cast(cast(0 as decimal(10,0)) as decimal(18,2)))) THEN 0 ELSE vip_level#32 END AS recharge_level#60]
                                             +- Project [user_id#34L, CASE WHEN isnull(vip_level#51) THEN 0 ELSE cast(vip_level#51 as int) END AS vip_level#32, CASE WHEN isnull(total_recharge_money#38) THEN cast(0.00 as decimal(18,2)) ELSE total_recharge_money#38 END AS total_recharge_money#33]
                                                +- Filter ((s_app_key#54 = wf_fish) &amp;&amp; (p_day#55 = 20191229))
                                                   +- SubqueryAlias fish_user_basic_data
                                                      +- CatalogRelation `dw_dmp`.`fish_user_basic_data`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [user_id#34L, register_time#35, phone_type#36, register_channel#37, total_recharge_money#38, balance_gold_money#39, balance_diamond_money#40, last_system_version#41, last_app_version#42, last_login_channel#43, last_create_order_channel#44, last_inventory_channel#45, last_bet_channel#46, last_active_time#47, user_type#48, battery_grade#49, recharge_count_num#50, vip_level#51, total_inventory_money#52, total_login_days#53], [s_app_key#54, p_day#55]
</code></pre><p>&#x62A5;&#x9519;&#x7684;&#x4EE3;&#x7801;&#x662F;&#xFF1A;&#x6570;&#x636E;&#x96C6;a&#x548C;&#x6570;&#x636E;&#x96C6;b&#x7528;&#x6761;&#x4EF6;xa&#x8FDB;&#x884C;join&#xFF0C;&#x7528;&#xFF08;&#x7ECF;&#x8FC7;&#x7B80;&#x5355;&#x7684;&#x903B;&#x8F91;&#x3001;&#x6570;&#x5B66;&#x8BA1;&#x7B97;&#xFF09;&#x4EA7;&#x5347;&#x7684;&#x6570;&#x636E;&#x96C6;c&#x548C;&#x6570;&#x636E;&#x96C6;d&#x3001;e&#x7528;&#x76F8;&#x540C;&#x7684;&#x6761;&#x4EF6;xa&#x8FDB;&#x884C;join&#xFF1B;&#x7ED3;&#x679C;&#x5C31;&#x62A5;&#x9519;<strong>resolved attribute(s) xxxx missing from...</strong>&#x4E86;&#x3002;&#x5176;&#x4E2D;xxxx&#x662F;&#x6761;&#x4EF6;xa&#x4E2D;&#x7684;&#x67D0;&#x4E00;&#x4E2A;&#x5217;&#xFF08;&#x8FD9;&#x4E00;&#x5217;&#x5728;&#x6570;&#x636E;&#x96C6;b&#x4E2D;&#x4F7F;&#x7528;as&#x53D6;&#x4E86;&#x522B;&#x540D;&#xFF0C;&#x4EE5;&#x8FDB;&#x884C;join&#xFF09;&#x3002;</p>
<p>&#x907F;&#x514D;&#x9519;&#x8BEF;&#x7684;&#x4EE3;&#x7801;&#x662F;&#xFF1A;&#x76F4;&#x63A5;&#x7528;&#x6570;&#x636E;&#x96C6;a&#x548C;&#x6570;&#x636E;&#x96C6;b&#xFF0C;d&#xFF0C;e&#x7528;&#x6761;&#x4EF6;xa&#x8FDB;&#x884C;join&#xFF0C;&#x518D;&#x8FDB;&#x884C;&#xFF08;&#x90A3;&#x4E9B;&#x7B80;&#x5355;&#x7684;&#x903B;&#x8F91;&#x3001;&#x6570;&#x5B66;&#x8BA1;&#x7B97;&#xFF09;&#x3002;</p>
<p>&#x7F51;&#x4E0A;&#x67E5;&#x5230;&#x7684;&#x53EF;&#x80FD;&#x7684;&#x539F;&#x56E0;&#x662F;&#xFF1A;&#x6570;&#x636E;&#x96C6;&#x7684;&#x5F15;&#x7528;&#x7684;&#x91CD;&#x7528;&#x4F1A;&#x5BFC;&#x81F4;&#x547D;&#x540D;&#x7684;&#x6A21;&#x7CCA;&#xFF1B;&#x611F;&#x89C9;&#x89E3;&#x91CA;&#x7684;&#x4E0D;&#x591F;&#x6709;&#x8BF4;&#x670D;&#x529B;&#x3002;&#x4E0D;&#x77E5;&#x9053;&#x662F;&#x4E0D;&#x662F;&#x6570;&#x636E;&#x96C6;b<strong>&#x8D77;&#x522B;&#x540D;</strong>&#x5BFC;&#x81F4;&#x4E86;&#x547D;&#x540D;&#x4E0A;&#x7684;&#x95EE;&#x9898;&#x3002;&#x67E5;&#x770B;&#x4E86;&#x4E00;&#x4E0B;&#x62A5;&#x9519;&#xFF1A;&#x62A5;&#x9519;&#x4E22;&#x5931;&#x7684;&#x662F;<em>recharge_level#738</em>&#xFF0C;join&#x6761;&#x4EF6;&#x89E3;&#x6790;&#x7684;&#x7ED3;&#x679C;&#x662F;&#xFF1A;</p>
<pre><code>((app_id#84L = app_id#825L) &amp;&amp; (channel_id#80L = channel_id#782L)) &amp;&amp; (is_parent_channel#65 = is_parent_channel#861)) &amp;&amp; (recharge_level#113 = recharge_level#738))
</code></pre><p>&#x5176;&#x4E2D;<em>recharge_level#</em>&#x540E;&#x9762;&#x7684;&#x6570;&#x636E;&#x4E0D;&#x540C;&#x7684;&#x6709;&#x51E0;&#x5904;&#xFF0C;&#x4F46;&#x662F;&#xFF0C;&#x67E5;&#x770B;&#x4E86;&#x4FEE;&#x6539;&#x540E;&#x6B63;&#x5E38;&#x7684;sql&#xFF0C;&#x4E5F;&#x6709;&#x5F88;&#x591A;&#x4E0D;&#x540C;&#x7684;<em>recharge_level#</em>&#x3002;</p>
<h3 id="impala&#x548C;hive&#x67E5;&#x8BE2;&#x7ED3;&#x679C;&#x4E0D;&#x4E00;&#x81F4;&#xFF01;&#xFF01;"><strong>Impala&#x548C;Hive&#x67E5;&#x8BE2;&#x7ED3;&#x679C;&#x4E0D;&#x4E00;&#x81F4;&#xFF01;&#xFF01;</strong></h3>
<p>&#x540E;&#x6765;&#x53D1;&#x73B0;&#xFF0C;&#x67E5;&#x8BE2;&#x524D;&#x4E00;&#x5929;&#x7684;&#x5206;&#x533A;&#x7684;&#x65F6;&#x5019;&#xFF0C;&#x7ED3;&#x679C;&#x662F;&#x4E00;&#x6837;&#x7684;&#xFF1B;&#x539F;&#x6765;&#x67E5;&#x8BE2;&#x7684;&#x662F;&#x5B9E;&#x65F6;&#x7684;&#x57CB;&#x70B9;&#x8868;&#xFF0C;&#x60F3;&#x5230;&#x4E86;&#x6267;&#x884C;<code>REFRESH</code>&#x547D;&#x4EE4;&#xFF0C;&#x6267;&#x884C;&#x4E86;&#x4E4B;&#x540E;&#xFF0C;&#x7ED3;&#x679C;&#x5C31;&#x4E00;&#x6837;&#x4E86;&#x3002;&#x6240;&#x4EE5;&#xFF0C;<strong>Impala&#x67E5;&#x8BE2;&#x5B9E;&#x65F6;&#x8868;&#xFF0C;&#x6700;&#x597D;&#x8981;&#x5148;&#x6267;&#x884C;</strong><code>REFRESH</code>&#x3002;</p>
<h3 id="hive-insert&#x987A;&#x5E8F;">Hive insert&#x987A;&#x5E8F;</h3>
<p>&#x5BF9;Hive&#x8868;&#x6267;&#x884C;<code>insert</code>&#x6216;&#x8005;<code>insert overwrite</code>&#x65F6;&#xFF0C;&#x6570;&#x636E;&#x7684;&#x5B57;&#x6BB5;&#x987A;&#x5E8F;<strong>&#x5FC5;&#x987B;</strong>&#x548C;<strong>Hive&#x8868;&#x7ED3;&#x6784;</strong>&#x7684;&#x5B57;&#x6BB5;&#x5C5E;&#x6027;<strong>&#x5B8C;&#x5168;&#x4E00;&#x81F4;</strong>&#x3002;&#x4E0D;&#x7136;&#x4F1A;&#x5BFC;&#x81F4;&#x6570;&#x636E;&#x7684;&#x9519;&#x4F4D;&#x3002;</p>
<h3 id="hive&#x4E34;&#x65F6;&#x6587;&#x4EF6;">Hive&#x4E34;&#x65F6;&#x6587;&#x4EF6;</h3>
<p>&#x901A;&#x8FC7;hive-sql&#x3001;hue&#x3001;spark-sql&#x7B49;&#x63D0;&#x4EA4;Hive&#x8868;&#x7684;<code>select</code>&#x6216;&#x8005;<code>insert overwirte</code>&#x65F6;&#xFF0C;&#x4F1A;&#x4EA7;&#x751F;&#x4E34;&#x65F6;&#x6587;&#x4EF6;&#x76EE;&#x5F55;<code>.hive-staging_hive_xxxxxxx</code>&#xFF0C;&#x7528;&#x4E8E;&#x5B58;&#x653E;&#x4E34;&#x65F6;&#x7684;&#x6267;&#x884C;&#x7ED3;&#x679C;&#xFF0C;&#x5BF9;&#x4E8E;&#x7528;&#x6237;&#x8FD9;&#x4E2A;&#x4E34;&#x65F6;&#x76EE;&#x5F55;&#x662F;&#x4E0D;&#x80FD;&#x8BBF;&#x95EE;&#x7684;&#xFF1B;&#x6BD4;&#x5982;<code>insert overwrite</code>&#x4F1A;&#x5C06;&#x7ED3;&#x679C;&#x6682;&#x5B58;&#x5728;&#x8BE5;&#x76EE;&#x5F55;&#x4E0B;&#xFF0C;&#x5F85;&#x4EFB;&#x52A1;&#x6267;&#x884C;&#x7ED3;&#x675F;&#xFF0C;&#x5C06;&#x7ED3;&#x679C;&#x590D;&#x5236;&#x5230;hive&#x8868;&#x4E2D;&#x3002;</p>
<p><img src="assets/1584669929700.png" alt=""></p>
<p>&#x8BE5;&#x4E34;&#x65F6;&#x6587;&#x4EF6;&#x7684;&#x9ED8;&#x8BA4;&#x76EE;&#x5F55;&#x4E3A;&#xFF0C;&#x64CD;&#x4F5C;&#x7684;hive&#x8868;&#x7684;&#x6839;&#x76EE;&#x5F55;&#xFF0C;&#x5373;&#x9ED8;&#x8BA4;&#x914D;&#x7F6E;&#x4E3A;&#xFF1A;</p>
<pre><code class="lang-xml"><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hive.exec.stagingdir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>.hive-staging<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>
</code></pre>
<p>&#x53EF;&#x4EE5;&#x5BF9;&#x5176;&#x8FDB;&#x884C;&#x4FEE;&#x6539;&#xFF0C;&#x4EE5;&#x7EDF;&#x4E00;&#x7BA1;&#x7406;&#x8BE5;&#x4E34;&#x65F6;&#x6587;&#x4EF6;&#xFF1A;</p>
<pre><code class="lang-xml"><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>
     <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hive.exec.stagingdir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>
     <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/tmp/hive/.hive-staging<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span>
</code></pre>
<p>&#x8BE5;&#x914D;&#x7F6E;&#x5BF9;Hive&#x6709;&#x6548;&#xFF0C;&#x5BF9;spark-sql&#x4E0D;&#x8D77;&#x4F5C;&#x7528;&#xFF0C;&#x8C8C;&#x4F3C;&#x662F;spark-sql&#x7684;bug&#x3002;</p>
<p>&#x4E00;&#x822C;&#xFF0C;<em>hive-staging</em>&#x6587;&#x4EF6;&#x4F1A;&#x81EA;&#x52A8;&#x5220;&#x9664;&#xFF1B;&#x4E24;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;<em>hive-staging</em>&#x6587;&#x4EF6;&#x4E0D;&#x4F1A;&#x81EA;&#x52A8;&#x5220;&#x9664;&#xFF1A;</p>
<ol>
<li>&#x4EFB;&#x52A1;&#x6267;&#x884C;&#x8FC7;&#x7A0B;&#x4E2D;&#x51FA;&#x73B0;&#x5F02;&#x5E38;&#xFF1B;</li>
<li>&#x957F;&#x65F6;&#x95F4;&#x4FDD;&#x5B58;&#x8FDE;&#x63A5;&#x6216;&#x8005;&#x4F1A;&#x8BDD;&#x3002;</li>
</ol>
<p>&#x5982;&#x679C;&#x4E0D;&#x80FD;&#x81EA;&#x52A8;&#x5220;&#x9664;&#x4E34;&#x65F6;&#x6587;&#x4EF6;&#xFF0C;&#x53EF;&#x4EE5;&#x8003;&#x8651;&#x4F7F;&#x7528;shell&#x811A;&#x672C;&#x5B9A;&#x65F6;&#x8FDB;&#x884C;&#x6E05;&#x7406;&#x3002;</p>
<h3 id="cannot-overwrite-a-path-that-is-also-being-read-from">Cannot overwrite a path that is also being read from.</h3>
<p>&#x521B;&#x5EFA;&#x4E86;&#x4E00;&#x4E2A;&#x5B58;&#x50A8;&#x683C;&#x5F0F;&#x4E3A;parquet&#x7684;Hive&#x8868;&#xFF0C;&#x4F7F;&#x7528;Spark SQL&#x8BFB;&#x53D6;&#x8868;&#x4E2D;&#x7684;&#x6570;&#x636E;&#x8FDB;&#x884C;&#x5904;&#x7406;&#x7136;&#x540E;&#x548C;&#x8868;&#x8FDB;&#x884C;<code>UNION</code>&#x521B;&#x5EFA;&#x4E34;&#x65F6;&#x8868;&#xFF0C;&#x518D;&#x901A;&#x8FC7;<code>INSERT OVERWIRTE TABLE ...</code>&#x628A;&#x4E34;&#x65F6;&#x8868;&#x6570;&#x636E;&#x63D2;&#x5165;&#x5230;&#x8FD9;&#x4E2A;Hive&#x8868;&#xFF0C;&#x7ED3;&#x679C;&#x62A5;&#x4EE5;&#x4E0B;&#x9519;&#x8BEF;&#xFF1A;</p>
<pre><code>20/04/26 17:47:42 ERROR yarn.ApplicationMaster: User class threw exception: org.apache.spark.sql.AnalysisException: Cannot overwrite a path that is also being read from.;
org.apache.spark.sql.AnalysisException: Cannot overwrite a path that is also being read from.;
    at org.apache.spark.sql.execution.datasources.DataSourceAnalysis 
    at ...
</code></pre><p>&#x5728;&#x7F51;&#x4E0A;&#x641C;&#x7D22;&#x539F;&#x56E0;&#xFF0C;&#x6709;&#x4E9B;&#x8BF4;&#x662F;&#x901A;&#x8FC7;Hive SQL&#x53EF;&#x4EE5;&#xFF0C;&#x4F46;&#x662F;Spark SQL&#x4E0D;&#x884C;&#xFF1B;&#x8FD9;&#x4E2A;&#x6CA1;&#x6709;&#x5C1D;&#x8BD5;&#x3002;</p>
<p>&#x4E00;&#x822C;&#x63D0;&#x4F9B;&#x7684;&#x89E3;&#x51B3;&#x65B9;&#x6848;&#x662F;&#x5148;&#x5199;&#x5165;&#x4E00;&#x4E2A;&#x4E34;&#x65F6;&#x8868;&#x4F5C;&#x4E3A;&#x4E2D;&#x95F4;&#x8FC7;&#x7A0B;&#xFF0C;&#x518D;&#x4ECE;&#x4E34;&#x65F6;&#x8868;&#x8986;&#x76D6;&#x76EE;&#x6807;&#x8868;&#x3002;</p>
<p>&#x770B;&#x5230;&#x6709;&#x4E00;&#x4E2A;&#x5C06;Hive&#x8868;&#x683C;&#x5F0F;&#x6539;&#x4E3A;orc&#x540E;&#x53EF;&#x4EE5;&#x6267;&#x884C;&#x7684;&#x7C7B;&#x4F3C;&#x5E16;&#x5B50;&#xFF0C;&#x5C31;&#x628A;&#x8868;&#x7684;&#x5B58;&#x50A8;&#x683C;&#x5F0F;&#x6539;&#x4E3A;&#x4E86;textfile&#xFF0C;&#x7136;&#x540E;&#x5C31;&#x53EF;&#x4EE5;&#x4E86;&#x3002;</p>
<p>&#x8FD8;&#x770B;&#x5230;&#x6709;&#x4ECB;&#x7ECD;&#x901A;&#x8FC7;spark&#x7684;checkpoint&#xFF08;checkpoint breaks data lineage&#xFF09;&#x6765;&#x89E3;&#x51B3;&#x7C7B;&#x4F3C;&#x95EE;&#x9898;&#x7684;&#xFF0C;&#x4E0D;&#x4E86;&#x89E3;checkpoint&#x6CA1;&#x6709;&#x5C1D;&#x8BD5;&#x3002;</p>
<h4 id="shutdown-hook-called-before-final-status-was-reported">Shutdown hook called before final status was reported.</h4>
<p>&#x62A5;&#x8FD9;&#x4E2A;&#x9519;&#xFF0C;&#x6709;&#x65F6;&#x5019;&#x662F;&#x56E0;&#x4E3A;&#x7A0B;&#x5E8F;&#x6709;&#x5F02;&#x5E38;&#xFF1B;</p>
<p>&#x5728;spark job&#x63D0;&#x4EA4;&#x540E;&#x975E;&#x6B63;&#x5E38;&#x7684;&#x505C;&#x6B62;&#x8BE5;job&#xFF0C;&#x6BD4;&#x5982;shell&#x63D0;&#x4EA4;&#x6267;&#x884C;&#xFF0C;&#x53C8;&#x7528;<code>Ctrl+C</code>&#x505C;&#x6B62;job&#xFF0C;&#x4E5F;&#x4F1A;&#x62A5;&#x8FD9;&#x4E2A;&#xFF1B;</p>
<p>&#x53E6;&#x5916;&#x5982;&#x679C;&#x5728;Spark&#x4EE3;&#x7801;&#x4E2D;&#x4E3A;&#x4E86;&#x7ED3;&#x675F;&#x7A0B;&#x5E8F;&#xFF0C;&#x52A0;&#x5165;&#x4E86;<code>System.exit(0)</code>&#xFF0C;&#x4E0D;&#x7BA1;&#x662F;&#x52A0;&#x5728;&#x751F;&#x6210;<code>SparkContext</code>&#x4E4B;&#x524D;&#xFF0C;&#x8FD8;&#x662F;&#x5728;&#x8C03;&#x7528;&#x4E86;<code>SparkSession.stop()</code>&#x4E4B;&#x540E;&#xFF1B;&#x5F53;&#x5728;yarn cluster&#x6267;&#x884C;&#x65F6;&#x90FD;&#x4F1A;&#x6709;&#x8BE5;&#x9519;&#x8BEF;&#x6216;&#x8005;&#x63D0;&#x793A;&#x3002;</p>
<pre><code>20/05/09 11:18:31 INFO yarn.ApplicationMaster: Final app status: FAILED, exitCode: 16, (reason: Shutdown hook called before final status was reported.)
20/05/09 11:18:31 INFO util.ShutdownHookManager: Shutdown hook called
</code></pre><p>&#x5927;&#x6982;&#x662F;Spark&#x5728;&#x7ED3;&#x675F;&#x4F5C;&#x4E1A;&#x4E4B;&#x524D;&#x4F1A;&#x8FDB;&#x884C;&#x4E00;&#x7CFB;&#x5217;&#x7684;&#x5904;&#x7406;&#xFF0C;&#x6BD4;&#x5982;&#x4E0A;&#x62A5;&#x4F5C;&#x4E1A;&#x6700;&#x7EC8;&#x72B6;&#x6001;&#xFF0C;&#x5220;&#x9664;staging&#x76EE;&#x5F55;<code>/.sparkStaging/application_1577764249201_982227</code>&#x7B49;&#x7B49;&#xFF1B;&#x5982;&#x679C;&#x6CA1;&#x6709;&#x5B8C;&#x6210;&#x8FD9;&#x4E9B;&#x5904;&#x7406;&#x5C31;&#x4F1A;&#x62A5;&#x8FD9;&#x4E2A;&#x6D88;&#x606F;&#x3002;</p>
<p>&#x7528;<code>return</code>&#x5173;&#x952E;&#x5B57;&#x53EF;&#x4EE5;&#x63D0;&#x524D;&#x7ED3;&#x675F;spark job&#xFF0C;&#x7ED3;&#x679C;&#x72B6;&#x6001;&#x8FD8;&#x662F;success&#x3002;</p>
<h3 id="&#x8FC7;&#x591A;&#x7684;&#x6267;&#x884C;&#x8BA1;&#x5212;&#x5BFC;&#x81F4;oom&#xFF1F;&#xFF1F;"><strong>&#x8FC7;&#x591A;&#x7684;&#x6267;&#x884C;&#x8BA1;&#x5212;&#x5BFC;&#x81F4;OOM&#xFF1F;&#xFF1F;</strong></h3>
<pre><code> &#x5E73;&#x53F0;&#x4E0A;&#x6267;&#x884C;&#x590D;&#x6742;&#x67E5;&#x8BE2;&#xFF0C;OOM&#xFF0C;&#x6839;&#x636E;&#x65E5;&#x5FD7;&#x63D0;&#x793A;&#x7684;&#x7ED3;&#x5C40;&#x65B9;&#x6CD5;&#xFF1A;

 -- SET spark.driver.memory=6/8G;&#x3010;&#x8FD8;&#x662F;OOM&#x3011;  
 set spark.sql.autoBroadcastJoinThreshold=-1;&#x3010;&#x89E3;&#x51B3;&#x95EE;&#x9898;&#x3011;
</code></pre><p><code>spark.sql.autoBroadcastJoinThreshold</code>&#x914D;&#x7F6E;&#x8FDB;&#x884C;join&#x65F6;&#x4E00;&#x4E2A;&#x8868;&#x80FD;&#x591F;&#x88AB;&#x5E7F;&#x64AD;&#x5230;&#x6240;&#x6709;&#x8282;&#x70B9;&#x7684;&#x6700;&#x5927;&#x7684;&#x6570;&#x91CF;&#xFF08;&#x5355;&#x4F4D;&#x5B57;&#x8282;&#xFF0C;&#x9ED8;&#x8BA4;&#x503C;10485760&#xFF0C;&#x5373;10M&#xFF09;&#x3002;&#x628A;&#x8FD9;&#x4E2A;&#x503C;&#x8BBE;&#x7F6E;&#x4E3A;<code>-1</code>&#x53EF;&#x4EE5;&#x7981;&#x7528;&#x5E7F;&#x64AD;&#x3002;&#x76EE;&#x524D;&#xFF0C;&#x53EA;&#x652F;&#x6301;&#x5DF2;&#x7ECF;&#x8FD0;&#x884C;&#x8FC7;<code>ANALYZE TABLE &lt;tableName&gt; COMPUTE STATISTICS noscan</code>&#x547D;&#x4EE4;&#x7684;Hive Metastore&#x8868;&#x3002;&#x8C8C;&#x4F3C;&#x8FD9;&#x4E2A;&#x914D;&#x7F6E;&#x662F;&#x5B9E;&#x9A8C;&#x6027;&#x7684;&#x914D;&#x7F6E;&#x9009;&#x9879;&#xFF08;experimental options&#xFF09;&#x3002;</p>
<h3 id="&#x548C;&#x7A7A;&#x7684;dataset&#x8FDB;&#x884C;&#x5185;&#x8FDE;&#x63A5;&#xFF0C;&#x7ED3;&#x679C;&#x62A5;&#x9519;&#x4E86;&#xFF1A;">&#x548C;&#x7A7A;&#x7684;Dataset&#x8FDB;&#x884C;&#x5185;&#x8FDE;&#x63A5;&#xFF0C;&#x7ED3;&#x679C;&#x62A5;&#x9519;&#x4E86;&#xFF1A;</h3>
<pre><code> User class threw exception: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:
Exchange SinglePartition 
.
.
.
</code></pre><p>&#x4E0A;&#x9762;&#x7684;&#x9519;&#x8BEF;&#x662F;&#x6700;&#x7EC8;&#x4F5C;&#x4E1A;&#x7ED3;&#x675F;&#x65F6;&#x7684;&#x62A5;&#x9519;&#xFF0C;&#x8FC7;&#x7A0B;&#x4E2D;&#x62A5;&#x7684;&#x9519;&#x8BEF;&#x662F;&#xFF1A;</p>
<pre><code>Exception in thread &quot;broadcast-exchange-1&quot; java.lang.OutOfMemoryError: Not enough memory to build and broadcast the table to all worker nodes.
</code></pre><p>&#x5947;&#x602A;&#xFF0C;&#x4E3A;&#x4EC0;&#x4E48;&#x548C;&#x7A7A;&#x7684;dataset&#x8FDB;&#x884C;join&#x4F1A;&#x62A5;&#x8FD9;&#x4E2A;&#x9519;&#x3002;</p>
<h4 id="spark-sql&#x7684;&#x6267;&#x884C;&#x662F;&#x4F9D;&#x8D56;hive-&#x8868;&#x7684;schema&#x7684;&#xFF0C;&#x867D;&#x7136;hive-&#x8868;&#x662F;&#x8BFB;&#x65F6;&#x6A21;&#x5F0F;&#xFF0C;&#x4F46;&#x662F;&#x4F7F;&#x7528;spark-sql&#x63D2;&#x5165;hive&#x8868;&#x65F6;&#xFF0C;&#x8FD8;&#x662F;&#x4F1A;&#x9A8C;&#x8BC1;&#x8868;&#x7684;schema&#x3002;">Spark SQL&#x7684;&#x6267;&#x884C;&#x662F;&#x4F9D;&#x8D56;Hive &#x8868;&#x7684;Schema&#x7684;&#xFF0C;&#x867D;&#x7136;Hive &#x8868;&#x662F;&#x8BFB;&#x65F6;&#x6A21;&#x5F0F;&#xFF0C;&#x4F46;&#x662F;&#x4F7F;&#x7528;Spark SQL&#x63D2;&#x5165;Hive&#x8868;&#x65F6;&#xFF0C;&#x8FD8;&#x662F;&#x4F1A;&#x9A8C;&#x8BC1;&#x8868;&#x7684;Schema&#x3002;</h4>
<h4 id="spark&#x4E2D;jdbc&#x66F4;&#x65B0;&#x6548;&#x7387;&#x5F88;&#x6162;&#x7684;&#x95EE;&#x9898;">Spark&#x4E2D;JDBC&#x66F4;&#x65B0;&#x6548;&#x7387;&#x5F88;&#x6162;&#x7684;&#x95EE;&#x9898;</h4>
<p>&#x5E94;&#x8BE5;&#x662F;&#x7D22;&#x5F15;&#x7684;&#x5173;&#x7CFB;&#xFF1B;&#x7ECF;&#x8FC7;&#x5B9E;&#x8DF5;&#xFF1A;&#x6539;&#x4E3A;&#x8054;&#x5408;&#x552F;&#x4E00;&#x7D22;&#x5F15;&#x540E;&#xFF0C;Spark&#x7A0B;&#x5E8F;&#x4E2D;&#x7684;update&#xFF08;where&#x6761;&#x4EF6;&#x5C31;&#x662F;&#x8054;&#x5408;&#x552F;&#x4E00;&#x7D22;&#x5F15;&#xFF09;&#x53D8;&#x5F97;&#x975E;&#x5E38;&#x5FEB;&#x3002;&#x4E0D;&#x77E5;&#x9053;&#x662F;&#x5426;&#x4E0E;&#x9501;&#x6709;&#x5173;&#x3002;Unique&#x9501;&#xFF0C;&#x4F1A;&#x4E0D;&#x4F1A;&#x5BFC;&#x81F4;&#x53EA;&#x9501;&#x4E00;&#x884C;&#xFF1F;&#x800C;&#x666E;&#x901A;&#x7D22;&#x5F15;&#x4F1A;&#x9501;&#x76F8;&#x540C;&#x7D22;&#x5F15;&#x7684;&#x884C;&#x5417;&#xFF1F;mysql&#x9ED8;&#x8BA4;&#x7684;autocommit&#x4E3A;true&#xFF0C;autocommit&#x4E3A;false&#x624D;&#x4F1A;&#x6709;&#x9501;&#x5417;&#xFF1F;</p>
<ul>
<li><p>User class threw exception: org.apache.spark.sql.AnalysisException: Can&apos;t extract value from c_operate_size#31 AS d_st_covershowtype#59: <strong>need struct type but got string</strong>;</p>
<p>&#x672C;&#x6765;&#x4E0D;&#x77E5;&#x9053;&#x4E3A;&#x4F55;&#x4F1A;&#x51FA;&#x73B0;&#x4E86;<code>struct type</code>&#xFF0C;&#x7ECF;&#x67E5;&#x770B;&#x4EE3;&#x7801;&#x53D1;&#x73B0;<code>&apos;c_operate_size.as(&quot;d_st_covershowtype&quot;)</code>&#x540E;&#x9762;&#x4E22;&#x5931;&#x4E86;&#x9017;&#x53F7;<code>,</code>&#x3002;&#x4E0D;&#x77E5;&#x4E3A;&#x4F55;&#x4F1A;&#x88AB;Spark&#x7406;&#x89E3;&#x4E3A;struct type&#x3002;</p>
<pre><code>&apos;c_operate_size.as(&quot;d_st_covershowtype&quot;)
      (unix_timestamp(&apos;d_act_time) * -1 + refSeconds).as(&quot;d_st_ctime&quot;),
</code></pre></li>
</ul>
<h4 id="alter-table-recover-partitions-only-works-on-table-with-location-provided">ALTER TABLE RECOVER PARTITIONS only works on table with location provided</h4>
<p>&#x521B;&#x5EFA;hive&#x5916;&#x90E8;&#x8868;&#xFF0C;&#x6709;&#x5206;&#x533A;<code>p_day</code>&#xFF0C;&#x4F7F;&#x7528;<code>SaveMode.Overwrite</code>&#x4FDD;&#x5B58;&#x6570;&#x636E;&#xFF0C;&#x671F;&#x671B;&#x80FD;&#x591F;&#x8986;&#x76D6;&#x6307;&#x5B9A;&#x7684;&#x5206;&#x533A;</p>
<pre><code>res.write.mode(SaveMode.Overwrite).partitionBy(&quot;p_day&quot;).format(&quot;parquet&quot;)
      .saveAsTable(&quot;db.table&quot;)
</code></pre><p>&#x62A5;&#x9519;&#xFF1A;</p>
<pre><code>User class threw exception: org.apache.spark.sql.AnalysisException: Operation not allowed: ALTER TABLE RECOVER PARTITIONS only works on table with location provided: `db`.`table`;
</code></pre><p>&#x770B;&#x4E86;HDFS&#xFF0C;&#x5206;&#x533A;&#x7684;&#x6587;&#x4EF6;&#x5DF2;&#x7ECF;&#x751F;&#x6210;&#xFF0C;&#x5728;Hive&#x4E2D;&#x6267;&#x884C;<code>ALTER TABLE ADD PARTITION(...)</code>&#x540E;&#xFF0C;&#x67E5;&#x770B;&#x6570;&#x636E;&#x4E5F;&#x662F;&#x6B63;&#x5E38;&#x7684;&#xFF1B;&#x95EE;&#x9898;&#x5E94;&#x8BE5;&#x662F;&#x51FA;&#x5728;&#x4E0D;&#x80FD;&#x6267;&#x884C;<code>ALTER TABLE RECOVER PARTITIONS</code>&#x3002;&#x540E;&#x6765;&#x53D1;&#x73B0;&#x8868;&#x53D8;&#x6210;&#x4E86;&#x5185;&#x90E8;&#x8868;&#xFF08;managed table&#xFF0C;&#x6839;&#x636E;&#x6570;&#x636E;&#x7684;schema&#x521B;&#x5EFA;&#x7684;&#xFF09;&#xFF0C;&#x770B;&#x6765;&#x662F;<code>SaveMode.Overwrite</code>&#x628A;&#x521B;&#x5EFA;&#x7684;&#x8868;&#x7ED9;&#x8986;&#x76D6;&#x4E86;&#xFF0C;&#x672C;&#x6765;&#x4EE5;&#x4E3A;&#x8FD9;&#x4E2A;&#x9009;&#x9879;&#x53EF;&#x4EE5;&#x53EA;&#x8986;&#x76D6;&#x5206;&#x533A;&#x7684;&#xFF0C;&#x539F;&#x6765;&#xFF0C;&#x5B83;&#x7684;&#x529F;&#x80FD;&#x662F;&#x8986;&#x76D6;&#x8868;&#x3002;&#x4F46;&#x662F;&#xFF0C;&#x65E2;&#x7136;Spark SQL&#x628A;&#x6570;&#x636E;&#x5DF2;&#x7ECF;&#x653E;&#x5165;HDFS&#xFF0C;&#x53EA;&#x9700;&#x628A;&#x5206;&#x533A;&#x4FE1;&#x606F;&#x52A0;&#x5230;Hive metastore&#xFF0C;&#x5E94;&#x8BE5;&#x5C31;&#x53EF;&#x4EE5;&#x4E86;&#x3002;</p>
<p>Spark 2.2.0&#x6709;&#x4E00;&#x4E2A;&#x65B9;&#x6CD5;<code>sparkSession.catalog.recoverPartitions()</code>&#xFF1A;</p>
<pre><code>  /**
   * Recovers all the partitions in the directory of a table and update the catalog.
   * Only works with a partitioned table, and not a view.
   *
   * @param tableName is either a qualified or unqualified name that designates a table.
   *                  If no database identifier is provided, it refers to a table in the
   *                  current database.
   * @since 2.1.1
   */
  def recoverPartitions(tableName: String): Unit
</code></pre><p>&#x6839;&#x636E;Hive RecoverPartitions&#x6587;&#x6863; <code>ALTER TABLE table_name RECOVER PARTITIONS;</code>&#x662F;&#x4E00;&#x4E2A;EMR&#xFF08;Amazon Elastic MapReduce&#xFF09;&#x64CD;&#x4F5C;&#x3002;&#x6B64;&#x5904;&#x4F7F;&#x7528;&#x7684;&#x662F;Hive&#xFF0C;&#x7B49;&#x4EF7;&#x7684;&#x64CD;&#x4F5C;&#x5E94;&#x8BE5;&#x662F;<code>MSCK [REPAIR] TABLE table_name [ADD/DROP/SYNC PARTITIONS];</code>&#xFF0C;&#x4E0D;&#x77E5;&#x9053;&#x662F;Spark SQL&#x5BF9;Hive&#x652F;&#x6301;&#x7684;bug&#xFF0C;&#x8FD8;&#x662F;Spark SQL&#x7684;&#x914D;&#x7F6E;&#x95EE;&#x9898;&#x3002;</p>
<h4 id="can-only-write-data-to-relations-with-a-single-path">Can only write data to relations with a single path.</h4>
<p>&#x5F80;Spark SQL&#x521B;&#x5EFA;&#x7684;&#x6258;&#x7BA1;&#x8868;&#x6267;&#x884C;<code>spark.sql(&quot;insert overwrite table...&quot;)</code>&#x6216;&#x8005;<code>spark.sql(&quot;insert into table...&quot;)</code>&#x62A5;&#x9519;&#xFF1A;</p>
<pre><code>org.apache.spark.sql.AnalysisException: Can only write data to relations with a single path.;
</code></pre><p>&#x4F46;&#x662F;&#x7528;Hive&#x521B;&#x5EFA;&#x6258;&#x7BA1;&#x8868;&#xFF0C;&#x5728;&#x6267;&#x884C;&#x5C31;&#x662F;&#x53EF;&#x4EE5;&#x7684;&#xFF0C;Hive&#x521B;&#x5EFA;&#x7684;&#x5916;&#x90E8;&#x8868;&#x4E5F;&#x662F;&#x53EF;&#x4EE5;&#x7684;&#x3002;</p>
<p>&#x4ECE;&#x62A5;&#x9519;&#x7684;stacktrace&#x67E5;&#x627E;&#xFF0C;&#x53D1;&#x73B0;&#x6E90;&#x7801;&#x6709;&#x4E00;&#x4E2A;sanity check&#xFF1A;</p>
<pre><code>      // Sanity checks
      if (t.location.rootPaths.size != 1) {
        throw new AnalysisException(
          &quot;Can only write data to relations with a single path.&quot;)
      }
</code></pre><p>&#x96BE;&#x9053;&#x662F;&#xFF0C;Spark SQL&#x521B;&#x5EFA;&#x7684;&#x5185;&#x90E8;&#x8868;<code>t.location.rootPaths.size</code>&#x4E3A;0&#xFF0C;&#x603B;&#x4E0D;&#x5E94;&#x8BE5;&#x5927;&#x4E8E;1&#x5427;&#xFF1F;&#x4F46;&#x662F;&#xFF0C;&#x5982;&#x679C;&#x8868;&#x4E0D;&#x5B58;&#x5728;&#x62A5;&#x7684;&#x9519;&#x5E94;&#x8BE5;&#x662F;<code>org.apache.spark.sql.AnalysisException: Table or view not found:xxxxxx</code>&#x3002;&#x800C;&#xFF0C;<code>spark.catalog.tableExists(&quot;db&quot;,&quot;table&quot;)</code>&#x6267;&#x884C;&#x7684;&#x7ED3;&#x679C;&#x53C8;&#x662F;<code>true</code>&#x3002;&#x8FD9;&#x4E2A;<code>location.rootPaths</code>&#x662F;&#x5565;&#xFF1F;&#x8D39;&#x89E3;&#x2026;&#x2026;</p>
<p>&#x5BF9;&#x6BD4;Spark SQL&#x521B;&#x5EFA;&#x7684;&#x5185;&#x90E8;&#x8868;&#x548C;Hive&#x521B;&#x5EFA;&#x7684;&#x5185;&#x90E8;&#x8868;&#xFF0C;&#x524D;&#x8005;&#x591A;&#x4E86;<code>spark.sql.sources.provider=parquet</code>&#x5E76;&#x4E14;<code>spark.sql.sources.schema.part.0</code>&#x4E2D;<code>fields</code>&#x4E2D;&#x7684;&#x5B57;&#x6BB5;&#x5C5E;&#x6027;<code>metadata</code>&#x662F;&#x7A7A;&#x7684;&#xFF0C;&#x800C;&#x540E;&#x8005;&#x4E0D;&#x662F;&#x7A7A;&#x7684;&#x3002;&#x4F46;&#x662F;&#x4F3C;&#x4E4E;&#x8FD9;&#x5E76;&#x4E0D;&#x8BE5;&#x662F;&#x5BFC;&#x81F4;&#x9519;&#x8BEF;&#x7684;&#x539F;&#x56E0;&#x3002;</p>
<h4 id="hive-&#x9501;&#x8868;&#xFF1F;">Hive &#x9501;&#x8868;&#xFF1F;</h4>
<p><a href="http://www.hplsql.org/home" target="_blank">http://www.hplsql.org/home</a></p>
<pre><code class="lang-sql"><span class="hljs-keyword">show</span> locks
<span class="hljs-keyword">unlock</span> <span class="hljs-keyword">table</span> &lt;table_name&gt;
</code></pre>
<p><strong>Spark application&#x5931;&#x8D25;,&#x4F46;&#x662F;Spark Web UI&#x4ECD;&#x7136;&#x6709;&#x6D3B;&#x8DC3;&#x7684;jobs</strong>&#xFF0C;Yarn application Web UI&#x663E;&#x793A;&#x7684;&#x72B6;&#x6001;&#x662F;Failed&#xFF0C;Spark Web UI&#x7684;bug&#xFF1F;&#xFF1F;</p>
<h4 id="spark&#x4F7F;&#x7528;todatestr&#x51FD;&#x6570;&#xFF0C;&#x6BD4;&#x4F7F;&#x7528;substringstr110&#x65E9;&#x4E86;&#x4E00;&#x5929;&#xFF0C;&#x4F8B;&#x5982;&#xFF0C;str&#x4E3A;2021-03-01-xxxxxx&#xFF0C;&#x524D;&#x8005;&#x7ED3;&#x679C;2021-02-28&#x3002;&#x5947;&#x602A;&#x4E86;&#x7528;&#x5230;&#x4E86;&#x65F6;&#x533A;&#x5417;&#xFF1F;&#xFF1F;&#xFF1F;&#x8FD8;&#x662F;&#x5C06;todatestr&#x4F5C;&#x4E3A;&#x8FDE;&#x63A5;&#x6761;&#x4EF6;&#x5BFC;&#x81F4;&#x4E86;&#x4EC0;&#x4E48;&#x53D8;&#x5316;&#xFF1F;&#xFF1F;">Spark&#x4F7F;&#x7528;<code>to_date(str)</code>&#x51FD;&#x6570;&#xFF0C;&#x6BD4;&#x4F7F;&#x7528;<code>substring(str,1,10)</code>&#x65E9;&#x4E86;&#x4E00;&#x5929;&#xFF0C;&#x4F8B;&#x5982;&#xFF0C;<code>str</code>&#x4E3A;<code>2021-03-01 xx:xx:xx</code>&#xFF0C;&#x524D;&#x8005;&#x7ED3;&#x679C;<code>2021-02-28</code>&#x3002;&#x5947;&#x602A;&#x4E86;&#x2026;&#x2026;&#x7528;&#x5230;&#x4E86;&#x65F6;&#x533A;&#x5417;&#xFF1F;&#xFF1F;&#xFF1F;&#x8FD8;&#x662F;&#x2026;&#x2026;&#x5C06;<code>to_date(str)</code>&#x4F5C;&#x4E3A;&#x8FDE;&#x63A5;&#x6761;&#x4EF6;&#x5BFC;&#x81F4;&#x4E86;&#x4EC0;&#x4E48;&#x53D8;&#x5316;&#xFF1F;&#xFF1F;</h4>
<h4 id="spark-structured-streaming&#x7684;&#x8F93;&#x51FA;&#x5EF6;&#x65F6;socket-&#x6570;&#x636E;&#x6E90;&#xFF0C;optionincludetimestamp-true&#xFF0C;&#x4F7F;&#x7528;&#x7A97;&#x53E3;&#x548C;&#x6C34;&#x5370;&#x7684;&#x805A;&#x5408;&#x3002;">Spark Structured Streaming&#x7684;&#x8F93;&#x51FA;&#x5EF6;&#x65F6;socket &#x6570;&#x636E;&#x6E90;&#xFF0C;<code>option(&quot;includeTimestamp&quot;, true)</code>&#xFF0C;&#x4F7F;&#x7528;&#x7A97;&#x53E3;&#x548C;&#x6C34;&#x5370;&#x7684;&#x805A;&#x5408;&#x3002;</h4>
<p>Spark WebUI &#x6709;&#x65F6;&#x5019;&#x770B;&#x5230;&#x6709;job &#x4E3A; <code>run at ThreadPoolExecutor.java:1149</code> &#xFF0C;&#x4E00;&#x76F4;&#x7591;&#x60D1;&#x662F;&#x4EC0;&#x4E48;&#xFF0C;&#x770B;StackOverflow&#x4E0A;&#x4E00;&#x4E2A;&#x56DE;&#x7B54;&#xFF0C;&#x4F3C;&#x4E4E;&#x662F;Spark join&#x65F6;&#xFF0C;&#x5982;&#x679C;&#x662F; broadcastjoin&#xFF0C;&#x4F1A;&#x8D77;&#x4E00;&#x4E2A;&#x7EBF;&#x7A0B;&#x53D1;&#x9001;&#x6570;&#x636E;&#x5230;executors&#x3002;&#x5927;&#x6982;&#x9700;&#x8981;&#x53D1;&#x9001;&#x6570;&#x636E;&#x5230;&#x5176;&#x4ED6;executors&#x65F6;&#x90FD;&#x4F1A;&#x53D1;&#x751F;&#x5427;&#xFF1F;&#xFF01;</p>
<p>&#x5F53;&#x7981;&#x7528;&#x4E86; broadcastjoin&#xFF08;&#x8BBE;&#x7F6E; <code>spark.sql.autoBroadcastJoinThreshold</code> &#x4E3A; -1&#xFF09;&#x540E;&#xFF0C;&#x7684;&#x786E;&#x5C31;&#x4E0D;&#x4F1A;&#x51FA;&#x73B0;&#x4E86;&#x3002;</p>
<h4 id="dataset-&#x7684;-repartition">Dataset &#x7684; <code>repartition()</code></h4>
<p><strong>df <code>repration()</code> &#x4E4B;&#x540E;&#x4FDD;&#x5B58;&#x4E3A;parquet&#x683C;&#x5F0F;&#x7684;hive&#x8868;</strong>&#xFF0C;&#x5927;&#x5C0F;&#x6BD4;&#x4E0D;&#x6267;&#x884C; <code>repartition()</code>&#x4F1A;&#x53D8;&#x5927;&#x5F88;&#x591A;&#xFF08;&#x4E4B;&#x524D; 60M &#x4E4B;&#x540E;209M&#xFF09;&#xFF0C;&#x4E0D;&#x77E5;&#x9053;&#x662F;&#x4E0D;&#x662F;&#x56E0;&#x4E3A; <code>repartion()</code> &#x64CD;&#x4F5C;&#x6309;&#x7167; roundrobin &#x7684;&#x65B9;&#x5F0F;&#x91CD;&#x65B0;&#x4E3A;&#x6DF7;&#x6D17;&#x4E4B;&#x540E;&#x7684;&#x6570;&#x636E;&#x8FDB;&#x884C;&#x5206;&#x533A;&#xFF0C;&#x5BFC;&#x81F4;&#x6570;&#x636E;&#x4E0D;&#x591F;&#x7D27;&#x51D1;&#xFF0C;&#x4ECE;&#x800C;&#x5BFC;&#x81F4; Parquet &#x7F16;&#x7801;&#x540E;&#x7684;&#x6587;&#x4EF6;&#x5360;&#x7528;&#x7A7A;&#x95F4;&#x53D8;&#x5927;&#x3002;</p>
<p>&#x7F3A;&#x7701;&#x53C2;&#x6570;&#x7684;<code>repartion()</code> &#x662F; <code>RoundRobinPartitioning</code>&#xFF0C;&#x5F53;&#x6307;&#x5B9A;&#x7684;&#x5206;&#x533A;&#x5217;&#xFF08;&#x5217;&#x7C7B;&#x578B;&#x5E94;&#x8BE5;&#x4E3A; <code>nonSortOrder</code> &#x7C7B;&#x578B;&#xFF09;&#x65F6;&#x6267;&#x884C;&#x7684;&#x662F; <code>HashPartitioning</code>&#x3002;</p>
<p><code>repartitionByRange(numPartitions:Int, partitionExprs: Column*)</code> &#x5206;&#x533A;&#x5217;&#x4E3A; <code>SortOrder</code> &#x7C7B;&#x578B;&#xFF0C;&#x6267;&#x884C;&#x7684;&#x662F; <code>RangePartitioning</code>&#x3002;</p>
<p><code>repartition</code> &#x7F3A;&#x7701;&#x7684;&#x5206;&#x533A;&#x6570;&#x91CF;&#x4E3A; <code>spark.sql.shuffle.partitions</code>&#x3002;</p>
<h4 id="sparksqladaptiveenabled"><code>spark.sql.adaptive.enabled</code></h4>
<p>&#x542F;&#x7528;&#x8BE5;<a href="https://help.aliyun.com/document_detail/93157.html" target="_blank">&#x914D;&#x7F6E;</a>&#xFF0C;<a href="https://support-it.huawei.com/docs/en-us/fusioninsight-all/fusioninsight_hd_6.5.1_documentation/en-us_topic_0176046027.html" target="_blank">&#x81EA;&#x9002;&#x5E94;&#x6DF7;&#x6D17;&#x5206;&#x533A;&#x4E2A;&#x6570;</a>&#x540E;&#xFF0C;spark.sql(&quot;insert overwrite table ....&quot;) &#x4F1A;&#x4EA7;&#x751F;&#x591A;&#x4E2A; job&#xFF0C;&#x800C;&#x4E0D;&#x542F;&#x7528;&#x8BE5;&#x914D;&#x7F6E;&#x5219;&#x53EA;&#x6709;&#x4E00;&#x4E2A;job&#x3002;&#x4E0D;&#x77E5;&#x9053;&#x4E3A;&#x4EC0;&#x4E48;&#x2026;&#x2026;</p>
<h4 id="spark-submit-&#x7684;---jars-&#x6307;&#x5B9A;-jar-&#x5305;&#x7684;&#x4F18;&#x5148;&#x7EA7;"><code>spark-submit</code> &#x7684; <code>--jars</code> &#x6307;&#x5B9A; jar &#x5305;&#x7684;&#x4F18;&#x5148;&#x7EA7;</h4>
<p>&#x4F3C;&#x4E4E;&#x662F; <code>--jars</code> &#x4E2D;&#x5148;&#x6307;&#x5B9A;&#x7684; jar &#x5305;&#x4F18;&#x5148;&#x7EA7;&#x9AD8;&#xFF1B;&#x9047;&#x5230;&#x7684;&#x60C5;&#x51B5;&#x662F;&#x2014;&#x2014;&#x5148;&#x5236;&#x5B9A;&#x4E86;&#x9AD8;&#x7248;&#x672C;&#x7684; mysql-connector jar&#xFF0C;&#x540E;&#x6307;&#x5B9A;&#x7684; shaded jar &#x4E2D;&#x5305;&#x542B;&#x4E86;&#x4F4E;&#x7248;&#x672C;&#x7684; mysql-connector jar&#xFF0C;&#x751F;&#x6548;&#x7684;&#x662F;&#x9AD8;&#x7248;&#x672C;&#x7684; mysql-connector jar&#xFF1B;&#x5982;&#x679C;&#x8C03;&#x6362;&#x4E8C;&#x8005;&#x987A;&#x5E8F;&#x751F;&#x6548;&#x7684;&#x5C31;&#x662F;&#x4F4E;&#x7248;&#x672C;&#x7684; mysql-connector jar&#x3002;</p>
<h4 id="spark-&#x4FEE;&#x6539;-hive-&#x8868;&#x5185;&#x5BB9;&#xFF0C;impala-&#x80FD;&#x67E5;&#x5230;&#x6700;&#x65B0;&#x7684;&#x7ED3;&#x679C;&#xFF0C;&#x4F46;&#x662F;-hive-&#x67E5;&#x5230;&#x7684;&#x662F;&#x4FEE;&#x6539;&#x524D;&#x7684;&#x7ED3;&#x679C;&#xFF01;&#xFF01;">Spark &#x4FEE;&#x6539; Hive &#x8868;&#x5185;&#x5BB9;&#xFF0C;Impala &#x80FD;&#x67E5;&#x5230;&#x6700;&#x65B0;&#x7684;&#x7ED3;&#x679C;&#xFF0C;&#x4F46;&#x662F; Hive &#x67E5;&#x5230;&#x7684;&#x662F;&#x4FEE;&#x6539;&#x524D;&#x7684;&#x7ED3;&#x679C;&#xFF01;&#xFF01;</h4>
<p>&#x539F;&#x56E0;&#x4E0D;&#x660E;&#x2026;&#x2026;&#x5177;&#x4F53;&#x64CD;&#x4F5C;&#x662F;&#x589E;&#x52A0;&#x4E86; Hive &#x8868;&#x5B57;&#x6BB5;&#x3002;</p>
<h4 id="sparksessionsqlinsert-overwrite-table--&#x8FD4;&#x56DE;&#x7684;&#x7ED3;&#x679C;&#x662F;&#x7A7A;-dataframe&#x3002;"><code>sparksession.sql(&quot;insert overwrite table ...&quot;)</code> &#x8FD4;&#x56DE;&#x7684;&#x7ED3;&#x679C;&#x662F;&#x7A7A; DataFrame&#x3002;</h4>
<h4 id="sparksession-&#x53EA;&#x5B58;&#x5728;&#x4E8E;-driver-&#x4E2D;&#xFF0C;&#x53EA;&#x80FD;&#x5728;-driver-&#x4E2D;&#x8C03;&#x7528;-sparksession">SparkSession &#x53EA;&#x5B58;&#x5728;&#x4E8E; Driver &#x4E2D;&#xFF0C;&#x53EA;&#x80FD;&#x5728; Driver &#x4E2D;&#x8C03;&#x7528; SparkSession</h4>
<pre><code class="lang-scala">    <span class="hljs-keyword">val</span> modeMapAcc = spark.sparkContext.collectionAccumulator[util.<span class="hljs-type">Map</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>]](<span class="hljs-string">&quot;mode-map&quot;</span>)
    df.foreach(
        r =&gt; {
          modeMapAcc.add(<span class="hljs-keyword">new</span> util.<span class="hljs-type">HashMap</span>[<span class="hljs-type">String</span>, <span class="hljs-type">String</span>]() {
            put(r, sparkSession.sql(...).head().getAs[<span class="hljs-type">Any</span>](<span class="hljs-number">0</span>).toString)
          })
        }
      )
</code></pre>
<p>&#x5982;&#x679C; <code>df</code> &#x5206;&#x533A;&#x4E0D;&#x662F;&#x5168;&#x90FD;&#x5728; Driver &#x4E0A;&#x4FDD;&#x5B58;&#xFF0C;&#x90A3;&#x4E48;&#x6267;&#x884C; <code>foreach</code> &#x64CD;&#x4F5C;&#x65F6;&#xFF0C;&#x4F1A;&#x6709;&#x5728; Executor &#x4E2D;&#x8C03;&#x7528; <code>SparkSession</code>&#x7684;&#x60C5;&#x51B5;&#xFF0C;&#x90A3;&#x4E48;&#x5C06;&#x4F1A;&#x629B;&#x51FA;&#x5F02;&#x5E38;&#xFF1A;</p>
<pre><code class="lang-text">...
Caused by: java.lang.NullPointerException at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:142) at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:140) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:641) at
...
</code></pre>
<h4 id="javalangunsupportedoperationexception-parquetcolumnvaluesdictionaryplainvaluesdictionaryplainlongdictionary">java.lang.UnsupportedOperationException: parquet.column.values.dictionary.PlainValuesDictionary$PlainLongDictionary</h4>
<pre><code>Caused by: java.lang.UnsupportedOperationException: parquet.column.values.dictionary.PlainValuesDictionary$PlainLongDictionary
    at parquet.column.Dictionary.decodeToBinary(Dictionary.java:44)
    at org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToBinary(ParquetDictionary.java:51)
    at org.apache.spark.sql.execution.vectorized.WritableColumnVector.getUTF8String(WritableColumnVector.java:380)
    at ... ...
</code></pre><p>&#x4FEE;&#x6539;&#x4E86;&#x4FDD;&#x5B58;&#x683C;&#x5F0F;&#x4E3A; Parquet &#x7684;&#x5206;&#x533A;&#x7684; Hive &#x8868;&#x67D0;&#x4E2A;&#x5B57;&#x6BB5;&#x7684;&#x7C7B;&#x578B;&#xFF08;&#x6BD4;&#x5982; bigint &#x6539;&#x4E3A; string&#xFF09;&#xFF0C;&#x4E4B;&#x540E;&#x8BFB;&#x53D6;&#x591A;&#x4E2A;&#x5206;&#x533A;&#xFF08;&#x5305;&#x542B;&#x4E86;&#x4FEE;&#x6539;&#x524D;&#x548C;&#x4FEE;&#x6539;&#x540E;&#xFF09;&#xFF0C;&#x5C31;&#x62A5;&#x8FD9;&#x4E2A;&#x9519;&#x3002;&#x5C06;&#x4FEE;&#x6539;&#x524D;&#x7684;&#x5206;&#x533A;&#x6570;&#x636E;&#x91CD;&#x65B0;&#x8DD1;&#x4E00;&#x8FB9;&#x5C31;&#x884C;&#x4E86;&#x3002;&#x539F;&#x56E0;&#x662F; Parquet &#x68C0;&#x6D4B;&#x5230;&#x4E86;<strong>&#x4E0D;&#x540C;&#x5206;&#x533A;&#xFF08;&#x6587;&#x4EF6;&#xFF09;&#x5B57;&#x6BB5;&#x7684;&#x7C7B;&#x578B;&#x4E0D;&#x4E00;&#x81F4;</strong>&#xFF01;&#xFF01;</p>
<h5 id="&#x5982;&#x679C;&#x5BF9;-df-&#x8FDB;&#x884C;-cache&#xFF0C;dffirst-&#x7B97;&#x5B50;&#xFF0C;&#x53EA;&#x4F1A;&#x8BA9;&#x90E8;&#x5206;&#xFF08;&#x67D0;&#x4E00;&#x4E2A;&#xFF09;&#x5206;&#x533A;&#x5237;&#x5165;&#x7F13;&#x5B58;&#x5417;&#xFF1F;">&#x5982;&#x679C;&#x5BF9; <code>df</code> &#x8FDB;&#x884C; <code>cache()</code>&#xFF0C;<code>df.first()</code> &#x7B97;&#x5B50;&#xFF0C;&#x53EA;&#x4F1A;&#x8BA9;&#x90E8;&#x5206;&#xFF08;&#x67D0;&#x4E00;&#x4E2A;&#xFF09;&#x5206;&#x533A;&#x5237;&#x5165;&#x7F13;&#x5B58;&#x5417;&#xFF1F;</h5>
<h5 id="spark-&#x8BFB;&#x53D6;-hive-&#x65F6;&#xFF0C;&#x62A5;&#x9519;-caused-by-javaioioexception-not-a-file-hdfsnameservicedatadwxxxdbxxxxxxxxxxxxxxxxxxxxxxx4465xxx4465tmp07303">Spark &#x8BFB;&#x53D6; Hive &#x65F6;&#xFF0C;&#x62A5;&#x9519; Caused by: java.io.IOException: Not a file: hdfs://nameservice/data/dw/xxx.db/xxxxxxxx/xxxx=xx/xxx=xxx/xxx=4465/xxx_4465_tmp07303</h5>
<p>&#x5927;&#x6982; Spark &#x8BFB;&#x53D6; Hive &#x65F6;&#xFF0C;&#x4F7F;&#x7528;&#x7684;&#x662F; Hive &#x7684; Metastore&#xFF0C;&#x5982;&#x679C; MetaStore &#x4E2D;&#x7684;&#x5206;&#x533A;&#x3001;&#x8868;&#x5BF9;&#x5E94;&#x7684;&#x6587;&#x4EF6;&#x4E0D;&#x5B58;&#x5728;&#xFF0C;&#x5C31;&#x4F1A;&#x62A5;&#x9519;&#x3002;</p>
<p>&#x8FD9;&#x79CD;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x53EF;&#x80FD;&#x662F;&#x8868;&#xFF08;&#x5206;&#x533A;&#xFF09;&#x6587;&#x4EF6;&#x56E0;&#x4E3A;&#x88AB;&#x5220;&#x9664;&#x4E86;&#xFF1B;&#x89E3;&#x51B3;&#x65B9;&#x6CD5;&#x2014;&#x2014;&#x8981;&#x4E48;&#x6062;&#x590D;&#x88AB;&#x5220;&#x9664;&#x7684;&#x6587;&#x4EF6;&#xFF0C;&#x8981;&#x4E48;&#x5C06;&#x5BF9;&#x5E94;&#x7684;&#x5206;&#x533A;&#xFF08;&#x8868;&#xFF09;&#x4ECE; MetaStore &#x4E2D;&#x5220;&#x9664;&#xFF08;&#x6BD4;&#x5982; <code>ALTER TABLE XX DROP PARTION(XX=XX,XX=XX)</code>&#xFF09;&#x3002;</p>
<h5 id="spark-sql-&#x4E2D;&#x9664;&#x6CD5;&#xFF08;&#xFF09;&#x7684;&#x8FD0;&#x7B97;&#x7ED3;&#x679C;&#x662F;-double-&#x7C7B;&#x578B;&#x7684;&#x3002;&#x5982;&#x679C;&#x7528;-round-&#x4FDD;&#x7559;&#x591A;&#x4F4D;&#x5C0F;&#x6570;&#xFF0C;&#x53EF;&#x80FD;&#x4F1A;&#x5BFC;&#x81F4;&#x7528;&#x79D1;&#x5B66;&#x8BB0;&#x6570;&#x6CD5;&#x8868;&#x793A;&#xFF08;&#x6BD4;&#x5982;&#xFF0C;6e-4&#xFF09;&#xFF0C;&#x5728;&#x5F80;-mysql-&#x8FDB;&#x884C;-jdbc-&#x63D2;&#x5165;&#x65F6;&#xFF0C;&#x4F1A;&#x51FA;&#x73B0;-not-a-numberic-&#x4E4B;&#x7C7B;&#x7684;&#x9519;&#x8BEF;&#x3002;">Spark SQL &#x4E2D;&#x9664;&#x6CD5;&#xFF08;<code>/</code>&#xFF09;&#x7684;&#x8FD0;&#x7B97;&#x7ED3;&#x679C;&#x662F; <code>Double</code> &#x7C7B;&#x578B;&#x7684;&#x3002;&#x5982;&#x679C;&#x7528; <code>round</code> &#x4FDD;&#x7559;&#x591A;&#x4F4D;&#x5C0F;&#x6570;&#xFF0C;&#x53EF;&#x80FD;&#x4F1A;&#x5BFC;&#x81F4;&#x7528;&#x79D1;&#x5B66;&#x8BB0;&#x6570;&#x6CD5;&#x8868;&#x793A;&#xFF08;&#x6BD4;&#x5982;&#xFF0C;<code>6E-4</code>&#xFF09;&#xFF0C;&#x5728;&#x5F80; mysql &#x8FDB;&#x884C; jdbc &#x63D2;&#x5165;&#x65F6;&#xFF0C;&#x4F1A;&#x51FA;&#x73B0; <code>Not a Numberic...</code> &#x4E4B;&#x7C7B;&#x7684;&#x9519;&#x8BEF;&#x3002;</h5>
<p>&#x89E3;&#x51B3;&#x65B9;&#x5F0F;&#x662F;&#xFF0C;&#x8FDB;&#x884C;&#x5F3A;&#x5236;&#x8F6C;&#x6362;&#xFF0C;&#x5C06;&#x5176;&#x8F6C;&#x6362;&#x4E3A; <code>decimal</code> &#x7C7B;&#x578B;&#xFF1A;</p>
<pre><code class="lang-sql">df.select(&apos;percent_bulala.cast(&quot;deciaml(6, 4)&quot;).as(&quot;percent_bulala&quot;))
</code></pre>
<p>&#x5176;&#x4E2D; <code>decimal</code> &#x5FC5;&#x987B;&#x6307;&#x5B9A;&#x7CBE;&#x5EA6;&#x548C;&#x5C0F;&#x6570;&#x4F4D;&#x6570;&#xFF0C;&#x4E0D;&#x7136;&#x9ED8;&#x8BA4;&#x7684;&#x5C0F;&#x6570;&#x4F4D;&#x597D;&#x50CF;&#x662F; 0&#x3002;</p>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="data-analyse.html" class="navigation navigation-prev " aria-label="Previous page: 数据分析">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="questions/dag-or-something.html" class="navigation navigation-next " aria-label="Next page: Spark框架">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"问题记录","level":"1.17","depth":1,"next":{"title":"Spark框架","level":"1.17.1","depth":2,"path":"questions/dag-or-something.md","ref":"questions/dag-or-something.md","articles":[]},"previous":{"title":"数据分析","level":"1.16","depth":1,"path":"data-analyse.md","ref":"data-analyse.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":["katex"],"pluginsConfig":{"katex":{},"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"questions.md","mtime":"2022-06-06T06:04:29.775Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2022-07-21T05:38:04.067Z"},"basePath":".","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="gitbook/gitbook.js"></script>
    <script src="gitbook/theme.js"></script>
    
        
        <script src="gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

